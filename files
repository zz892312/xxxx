Slide 1: Opening
Title: Opening

Script:

"Good morning, everyone! Today, I’ll walk you through our findings on deploying machine learning models with KServe on OpenShift. We’ll cover what KServe is, why we’re using it, and how we’ve decided to deploy it—starting with a phased approach using RawDeployment mode."

Slide 2: KServe in 60 Seconds
Visual: Diagram — Models → KServe → Predictions

Script:

"Let’s start with the basics—what is KServe?
KServe is an open-source platform for serving machine learning models at scale, built on Kubernetes.
Think of it as a smart waiter for your models: it receives prediction requests, forwards them to your model, and returns the results quickly and reliably."

Slide 3: Core Capabilities & Analogy
Visual: Bulleted list of features
Script:

"KServe works with many model formats like PyTorch, TensorFlow, and ONNX.
It can scale automatically based on traffic, provide built-in monitoring and security, and standardize model deployment workflows.
If your ML model is a chef creating predictions, KServe is the restaurant operation:
It manages the kitchen (autoscaling), takes orders (API calls), ensures food safety (security), and monitors customer satisfaction (metrics)."

Slide 4: Why KServe Instead of DIY
Visual: Table comparing KServe vs. Custom Solutions
Script:

"Before KServe, each team built its own model serving stack.
Team A might’ve used Flask and Docker, Team B used FastAPI, Team C had something totally custom.
That created fragmentation, made scaling difficult, and observability inconsistent."

Slide 5: POC Validation
Visual: POC results with checkmarks
Script:

"In our proof-of-concept, we validated several key features:

Prometheus metrics were auto-instrumented with a single annotation

Internal mTLS worked out of the box via OpenShift Service Mesh

RBAC was fine-grained using Kubernetes service accounts
But we also uncovered a critical infrastructure issue..."

Slide 6: Why We're Avoiding Istio (For Now)
Visual: Diagram showing OpenShift native networking vs. Istio overlay
Script:

"KServe’s default serverless mode requires Istio, but that’s a risk for us right now.
If we enable Istio fully, it would take over our OpenShift Routes, DNS, and load balancing—forcing us to retest and rebuild everything.
Plus, the overhead of managing Istio is non-trivial—over 50 CRDs, version drift, and complexity for legacy apps."

Slide 7: What We Learned About KServe
Visual: Side-by-side comparison of Serverless vs. RawDeployment mode
Script:

"We evaluated both KServe deployment modes:

Serverless Mode uses Knative and Istio to offer scale-to-zero and advanced routing, but at the cost of extra resource usage and operational complexity.

Raw Deployment Mode lets us avoid Istio and Knative, use standard OpenShift networking, and scale with Kubernetes HPA.

Because of that, we’re starting with RawDeployment as our baseline—it’s simpler, predictable, and integrates cleanly with OpenShift."

