name: Deploy Grafana Dashboard for Triton Inference Server

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      dashboard_name:
        description: 'Dashboard name'
        required: false
        default: 'triton-inference-monitoring'

env:
  GRAFANA_URL: ${{ secrets.GRAFANA_URL }}
  GRAFANA_API_KEY: ${{ secrets.GRAFANA_API_KEY }}

jobs:
  deploy-dashboard:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install dependencies
      run: |
        npm install -g @grafana/toolkit

    - name: Validate Grafana URL and API Key
      run: |
        if [ -z "${{ env.GRAFANA_URL }}" ]; then
          echo "Error: GRAFANA_URL secret not set"
          exit 1
        fi
        if [ -z "${{ env.GRAFANA_API_KEY }}" ]; then
          echo "Error: GRAFANA_API_KEY secret not set"
          exit 1
        fi

    - name: Create dashboard directory
      run: mkdir -p grafana/dashboards

    - name: Generate Triton Dashboard JSON
      run: |
        cat > grafana/dashboards/triton-dashboard.json << 'EOF'
        {
          "dashboard": {
            "id": null,
            "title": "Triton Inference Server Monitoring",
            "tags": ["triton", "inference", "ml"],
            "timezone": "browser",
            "panels": [
              {
                "id": 1,
                "title": "Request Rate",
                "type": "stat",
                "targets": [
                  {
                    "expr": "rate(nv_inference_request_success_total[5m])",
                    "legendFormat": "Success Rate",
                    "refId": "A"
                  },
                  {
                    "expr": "rate(nv_inference_request_failure_total[5m])",
                    "legendFormat": "Failure Rate",
                    "refId": "B"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "reqps",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
              },
              {
                "id": 2,
                "title": "Request Duration",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "histogram_quantile(0.50, rate(nv_inference_request_duration_us_bucket[5m]))",
                    "legendFormat": "p50",
                    "refId": "A"
                  },
                  {
                    "expr": "histogram_quantile(0.95, rate(nv_inference_request_duration_us_bucket[5m]))",
                    "legendFormat": "p95",
                    "refId": "B"
                  },
                  {
                    "expr": "histogram_quantile(0.99, rate(nv_inference_request_duration_us_bucket[5m]))",
                    "legendFormat": "p99",
                    "refId": "C"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "Âµs",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
              },
              {
                "id": 3,
                "title": "Queue Time",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "histogram_quantile(0.50, rate(nv_inference_queue_duration_us_bucket[5m]))",
                    "legendFormat": "p50 Queue Time",
                    "refId": "A"
                  },
                  {
                    "expr": "histogram_quantile(0.95, rate(nv_inference_queue_duration_us_bucket[5m]))",
                    "legendFormat": "p95 Queue Time",
                    "refId": "B"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "Âµs",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
              },
              {
                "id": 4,
                "title": "Compute Time",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "histogram_quantile(0.50, rate(nv_inference_compute_duration_us_bucket[5m]))",
                    "legendFormat": "p50 Compute Time",
                    "refId": "A"
                  },
                  {
                    "expr": "histogram_quantile(0.95, rate(nv_inference_compute_duration_us_bucket[5m]))",
                    "legendFormat": "p95 Compute Time",
                    "refId": "B"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "Âµs",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
              },
              {
                "id": 5,
                "title": "GPU Utilization",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "nv_gpu_utilization",
                    "legendFormat": "GPU {{gpu_uuid}}",
                    "refId": "A"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "percent",
                    "min": 0,
                    "max": 100
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
              },
              {
                "id": 6,
                "title": "GPU Memory Usage",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "nv_gpu_memory_used_bytes / nv_gpu_memory_total_bytes * 100",
                    "legendFormat": "GPU {{gpu_uuid}} Memory",
                    "refId": "A"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "percent",
                    "min": 0,
                    "max": 100
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
              },
              {
                "id": 7,
                "title": "Model States",
                "type": "stat",
                "targets": [
                  {
                    "expr": "nv_model_inference_count",
                    "legendFormat": "{{model}} - {{version}}",
                    "refId": "A"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "short",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 0, "y": 24}
              },
              {
                "id": 8,
                "title": "Error Rate",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "rate(nv_inference_request_failure_total[5m]) / rate(nv_inference_request_success_total[5m] + nv_inference_request_failure_total[5m]) * 100",
                    "legendFormat": "Error Rate",
                    "refId": "A"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "percent",
                    "min": 0,
                    "max": 100
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 12, "y": 24}
              },
              {
                "id": 9,
                "title": "Concurrent Requests",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "nv_inference_pending_request_count",
                    "legendFormat": "Pending Requests",
                    "refId": "A"
                  },
                  {
                    "expr": "nv_inference_exec_count",
                    "legendFormat": "Executing Requests",
                    "refId": "B"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "short",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 0, "y": 32}
              },
              {
                "id": 10,
                "title": "Batch Size",
                "type": "timeseries",
                "targets": [
                  {
                    "expr": "nv_inference_batch_size",
                    "legendFormat": "Batch Size - {{model}}",
                    "refId": "A"
                  }
                ],
                "fieldConfig": {
                  "defaults": {
                    "unit": "short",
                    "min": 0
                  }
                },
                "gridPos": {"h": 8, "w": 12, "x": 12, "y": 32}
              }
            ],
            "time": {
              "from": "now-1h",
              "to": "now"
            },
            "refresh": "30s",
            "schemaVersion": 30,
            "version": 1,
            "editable": true
          },
          "folderId": null,
          "overwrite": true
        }
        EOF

    - name: Deploy Dashboard to Grafana
      run: |
        # Check if dashboard already exists
        DASHBOARD_TITLE="Triton Inference Server Monitoring"
        EXISTING_DASHBOARD=$(curl -s -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
          "${{ env.GRAFANA_URL }}/api/search?query=$DASHBOARD_TITLE" | jq -r '.[0].uid // empty')
        
        if [ ! -z "$EXISTING_DASHBOARD" ]; then
          echo "Dashboard exists with UID: $EXISTING_DASHBOARD"
          # Update existing dashboard
          curl -X POST \
            -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @grafana/dashboards/triton-dashboard.json \
            "${{ env.GRAFANA_URL }}/api/dashboards/uid/$EXISTING_DASHBOARD"
        else
          echo "Creating new dashboard"
          curl -X POST \
            -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @grafana/dashboards/triton-dashboard.json \
            "${{ env.GRAFANA_URL }}/api/dashboards/db"
        fi

    - name: Create Data Source (if not exists)
      run: |
        # Check if Prometheus data source exists
        DATASOURCE_NAME="prometheus"
        EXISTING_DS=$(curl -s -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
          "${{ env.GRAFANA_URL }}/api/datasources/name/$DATASOURCE_NAME" | jq -r '.id // empty')
        
        if [ -z "$EXISTING_DS" ]; then
          echo "Creating Prometheus data source"
          curl -X POST \
            -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d '{
              "name": "prometheus",
              "type": "prometheus",
              "url": "http://prometheus:9090",
              "access": "proxy",
              "basicAuth": false,
              "isDefault": true
            }' \
            "${{ env.GRAFANA_URL }}/api/datasources"
        else
          echo "Prometheus data source already exists with ID: $EXISTING_DS"
        fi

    - name: Setup Alerts (Optional)
      run: |
        # Create alert rules for critical metrics
        cat > grafana/alerts.json << 'EOF'
        {
          "title": "Triton High Error Rate",
          "condition": "A",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "model": {
                "expr": "rate(nv_inference_request_failure_total[5m]) / rate(nv_inference_request_success_total[5m] + nv_inference_request_failure_total[5m]) * 100 > 5"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Alerting",
          "for": "5m",
          "annotations": {
            "description": "Triton inference server error rate is above 5%",
            "summary": "High error rate detected"
          },
          "labels": {
            "severity": "warning"
          }
        }
        EOF
        
        # Deploy alert rule
        curl -X POST \
          -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
          -H "Content-Type: application/json" \
          -d @grafana/alerts.json \
          "${{ env.GRAFANA_URL }}/api/ruler/grafana/api/v1/rules/default"

    - name: Verify Dashboard Deployment
      run: |
        # Get dashboard URL
        DASHBOARD_URL=$(curl -s -H "Authorization: Bearer ${{ env.GRAFANA_API_KEY }}" \
          "${{ env.GRAFANA_URL }}/api/search?query=Triton%20Inference%20Server%20Monitoring" | \
          jq -r '.[0].url // empty')
        
        if [ ! -z "$DASHBOARD_URL" ]; then
          echo "âœ… Dashboard deployed successfully!"
          echo "Dashboard URL: ${{ env.GRAFANA_URL }}$DASHBOARD_URL"
        else
          echo "âŒ Dashboard deployment failed"
          exit 1
        fi

    - name: Create deployment summary
      run: |
        echo "## ðŸ“Š Grafana Dashboard Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Dashboard Details:" >> $GITHUB_STEP_SUMMARY
        echo "- **Name**: Triton Inference Server Monitoring" >> $GITHUB_STEP_SUMMARY
        echo "- **URL**: ${{ env.GRAFANA_URL }}/d/triton-inference-monitoring" >> $GITHUB_STEP_SUMMARY
        echo "- **Deployment Time**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Key Metrics Monitored:" >> $GITHUB_STEP_SUMMARY
        echo "- Request Rate & Success/Failure Rates" >> $GITHUB_STEP_SUMMARY
        echo "- Request Duration (p50, p95, p99)" >> $GITHUB_STEP_SUMMARY
        echo "- Queue Time & Compute Time" >> $GITHUB_STEP_SUMMARY
        echo "- GPU Utilization & Memory Usage" >> $GITHUB_STEP_SUMMARY
        echo "- Model States & Error Rates" >> $GITHUB_STEP_SUMMARY
        echo "- Concurrent Requests & Batch Sizes" >> $GITHUB_STEP_SUMMARY