# Triton Inference Server - MobileNet v2 GPU Deployment

## Directory Structure
```
model_repository/
└── mobilenet_v2/
    ├── config.pbtxt
    └── 1/
        └── model.onnx
```

## Step 1: Download the Model

```bash
# Create the model repository structure
mkdir -p model_repository/mobilenet_v2/1

# Download MobileNet v2 ONNX model
wget -O model_repository/mobilenet_v2/1/model.onnx \
  https://huggingface.co/qualcomm/MobileNet-v2/resolve/main/MobileNet-v2.onnx
```

## Step 2: Create Triton Configuration

Create `model_repository/mobilenet_v2/config.pbtxt`:

```protobuf
name: "mobilenet_v2"
platform: "onnxruntime_onnx"
max_batch_size: 8
version_policy: { all { }}

input [
  {
    name: "input"
    data_type: TYPE_FP32
    format: FORMAT_NCHW
    dims: [ 3, 224, 224 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

optimization {
  execution_accelerators {
    gpu_execution_accelerator : [ {
      name : "tensorrt"
      parameters { key: "precision_mode" value: "FP16" }
      parameters { key: "max_workspace_size_bytes" value: "1073741824" }
    }]
  }
}

dynamic_batching {
  max_queue_delay_microseconds: 500
}
```

## Step 3: Run Triton Server

```bash
# Using Docker (Recommended)
docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 \
  -v$(pwd)/model_repository:/models \
  nvcr.io/nvidia/tritonserver:24.01-py3 \
  tritonserver --model-repository=/models

# Or if you have Triton installed locally
tritonserver --model-repository=./model_repository
```

## Step 4: Test the Deployment

### Check Model Status
```bash
curl -v localhost:8000/v2/models/mobilenet_v2
```

### Health Check
```bash
curl -v localhost:8000/v2/health/ready
```

### Server Metadata
```bash
curl -v localhost:8000/v2
```

## Step 5: Client Code Example (Python)

```python
import tritonclient.http as httpclient
import numpy as np
from PIL import Image

# Initialize client
triton_client = httpclient.InferenceServerClient(url="localhost:8000")

# Prepare input data
def preprocess_image(image_path):
    image = Image.open(image_path).convert('RGB')
    image = image.resize((224, 224))
    image_array = np.array(image).astype(np.float32)
    # Normalize to [0,1] and convert to NCHW format
    image_array = image_array / 255.0
    image_array = np.transpose(image_array, (2, 0, 1))  # HWC to CHW
    image_array = np.expand_dims(image_array, axis=0)   # Add batch dimension
    return image_array

# Create input
input_data = preprocess_image("your_image.jpg")
inputs = [httpclient.InferInput("input", input_data.shape, "FP32")]
inputs[0].set_data_from_numpy(input_data)

# Create output
outputs = [httpclient.InferRequestedOutput("output")]

# Run inference
results = triton_client.infer(model_name="mobilenet_v2", inputs=inputs, outputs=outputs)

# Get predictions
predictions = results.as_numpy("output")[0]
predicted_class = np.argmax(predictions)
confidence = predictions[predicted_class]

print(f"Predicted class: {predicted_class}")
print(f"Confidence: {confidence:.4f}")
```

## Configuration Explanation

### Key Settings for GPU Deployment:

1. **`platform: "onnxruntime_onnx"`** - Uses ONNX Runtime backend with GPU support
2. **`kind: KIND_GPU`** - Forces GPU execution
3. **`gpus: [0]`** - Uses GPU 0 (adjust as needed)
4. **`max_batch_size: 8`** - Allows batching for better GPU utilization
5. **`tensorrt` optimization** - Enables TensorRT acceleration for better performance
6. **`dynamic_batching`** - Automatically batches requests for efficiency

### Model Details:
- **Input**: RGB image, 224x224 pixels, NCHW format (batch, channels, height, width)
- **Output**: 1000 class probabilities (ImageNet classes)
- **Batch Size**: Up to 8 images per inference call
- **Precision**: FP16 for faster GPU inference

## Performance Tips

1. **Batch Size**: Increase `max_batch_size` if you expect multiple concurrent requests
2. **TensorRT**: The config enables TensorRT optimization for better GPU performance
3. **Memory**: Adjust `max_workspace_size_bytes` based on your GPU memory
4. **Multiple GPUs**: Add more GPU IDs to the `gpus` array if you have multiple GPUs

## Troubleshooting

- **GPU not detected**: Ensure NVIDIA drivers and CUDA are properly installed
- **Model loading fails**: Check that the ONNX model is in the correct path
- **Permission errors**: Ensure Docker has access to the model directory
- **Memory issues**: Reduce batch size or disable TensorRT optimization

This setup provides a production-ready deployment of MobileNet v2 on GPU with optimal performance settings.
