Use the Service Mesh system and enable KServe serverless mode at the app-code-level namespace.
The inference namespaces are limited to just non-prod and prod.
This setup could allow us to natively support scale-to-zero for inference services without needing any additional implementation.


graph TD
    subgraph Non-Prod Cluster
        subgraph app_code_1
            A1[Dev] --> I
            B1[QA] --> I
            C1[UAT] --> I
        end
        I[Inference Namespace<br>KServe Serverless]
    end
    External -- MTLS --> Auth[Auth Server]
    Auth -->|Auth Token| B1
    B1 --> I

Use RawDeployment mode with no additional namespace setup.
Inference services are deployed alongside the application as part of the promotion process — either directly from dev/qa/uat to prod, depending on the team’s choice.
The research workflow is restricted to using only the -dev namespace.


flowchart TD
 subgraph s1["app_code_1-dev"]
        A1["App Service"]
        I1["KServe RawDeployment Pod"]
  end
 subgraph subGraph1["Non-Prod Cluster"]
        s1
  end
    A1 --> I1
    Route["OpenShift Route - with mTLS"] --> A1 & I1
    External["External Client"] -- mTLS --> Route



