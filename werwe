python llm_rps_benchmark.py \
  --target http://localhost:8000 \
  --rps 5000 \
  --duration 60 \
  --max-tokens 50
pip install aiohttp
#!/usr/bin/env python3
"""
LLM RPS Benchmark Script
Tests actual RPS against any OpenAI-compatible LLM server.
No ceiling — uses asyncio + multiprocessing to saturate the server.

Usage:
    python llm_rps_benchmark.py --target http://localhost:8000 --rps 5000 --duration 60
    python llm_rps_benchmark.py --target http://localhost:8000 --rps 1000 --duration 30 --workers 4
"""

import argparse
import asyncio
import json
import os
import random
import time
import multiprocessing
from collections import defaultdict
from dataclasses import dataclass, field
from typing import List, Optional

import aiohttp

# ─────────────────────────────────────────────
# Configuration
# ─────────────────────────────────────────────

DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."

SAMPLE_PROMPTS = [
    "Explain the concept of machine learning in simple terms and provide a basic example of how it works.",
    "What are the key differences between supervised and unsupervised learning algorithms?",
    "Describe the architecture of a transformer model and its role in natural language processing.",
    "How does gradient descent work in neural network training? Explain the process step by step.",
    "What is the difference between precision and recall in machine learning evaluation metrics?",
    "Explain the concept of overfitting in machine learning and how to prevent it.",
    "What are the main components of a convolutional neural network and their functions?",
    "Describe the process of tokenization in natural language processing.",
    "What is transfer learning and why is it useful in deep learning applications?",
    "Explain the attention mechanism in transformer models and its importance.",
    "What are the advantages and disadvantages of using large language models?",
    "Describe the differences between batch normalization and layer normalization.",
    "How does reinforcement learning differ from supervised learning? Give examples.",
    "What is the purpose of dropout regularization in neural networks?",
    "Explain the concept of embeddings in natural language processing.",
    "What are some common challenges in deploying machine learning models to production?",
    "Describe the role of activation functions in neural networks.",
    "What is the difference between LSTM and GRU architectures in recurrent neural networks?",
    "How does the BERT model work and what makes it different from GPT?",
    "Explain the concept of fine-tuning a pre-trained language model.",
]


# ─────────────────────────────────────────────
# Data Classes
# ─────────────────────────────────────────────

@dataclass
class RequestResult:
    success: bool
    status_code: int
    latency_ms: float           # E2E latency
    ttft_ms: float              # Time to first token
    output_tokens: int
    error: Optional[str] = None


@dataclass
class BenchmarkStats:
    total_requests: int = 0
    successful: int = 0
    failed: int = 0
    latencies_ms: List[float] = field(default_factory=list)
    ttfts_ms: List[float] = field(default_factory=list)
    output_tokens_list: List[int] = field(default_factory=list)
    errors: dict = field(default_factory=lambda: defaultdict(int))
    start_time: float = 0
    end_time: float = 0

    def actual_rps(self) -> float:
        duration = self.end_time - self.start_time
        return self.successful / duration if duration > 0 else 0

    def percentile(self, data: List[float], p: float) -> float:
        if not data:
            return 0
        sorted_data = sorted(data)
        idx = int(len(sorted_data) * p / 100)
        return sorted_data[min(idx, len(sorted_data) - 1)]


# ─────────────────────────────────────────────
# Request Sender
# ─────────────────────────────────────────────

async def send_request(
    session: aiohttp.ClientSession,
    target: str,
    model: str,
    prompt: str,
    max_tokens: int,
    timeout: int,
) -> RequestResult:
    url = f"{target}/v1/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        "max_tokens": max_tokens,
        "stream": True,   # streaming to capture TTFT
    }

    start = time.monotonic()
    ttft_ms = 0.0
    output_tokens = 0
    first_token = False

    try:
        async with session.post(
            url,
            json=payload,
            timeout=aiohttp.ClientTimeout(total=timeout),
        ) as resp:
            status = resp.status
            if status != 200:
                return RequestResult(
                    success=False,
                    status_code=status,
                    latency_ms=(time.monotonic() - start) * 1000,
                    ttft_ms=0,
                    output_tokens=0,
                    error=f"HTTP {status}",
                )

            async for raw_line in resp.content:
                line = raw_line.decode("utf-8").strip()
                if not line or not line.startswith("data:"):
                    continue
                data_str = line[len("data:"):].strip()
                if data_str == "[DONE]":
                    break
                try:
                    chunk = json.loads(data_str)
                    delta = chunk["choices"][0].get("delta", {})
                    if delta.get("content"):
                        if not first_token:
                            ttft_ms = (time.monotonic() - start) * 1000
                            first_token = True
                        output_tokens += 1
                except (json.JSONDecodeError, KeyError):
                    continue

        latency_ms = (time.monotonic() - start) * 1000
        return RequestResult(
            success=True,
            status_code=200,
            latency_ms=latency_ms,
            ttft_ms=ttft_ms,
            output_tokens=output_tokens,
        )

    except asyncio.TimeoutError:
        return RequestResult(
            success=False,
            status_code=0,
            latency_ms=(time.monotonic() - start) * 1000,
            ttft_ms=0,
            output_tokens=0,
            error="Timeout",
        )
    except aiohttp.ClientConnectorError as e:
        return RequestResult(
            success=False,
            status_code=0,
            latency_ms=(time.monotonic() - start) * 1000,
            ttft_ms=0,
            output_tokens=0,
            error=f"Connection error: {e}",
        )
    except Exception as e:
        return RequestResult(
            success=False,
            status_code=0,
            latency_ms=(time.monotonic() - start) * 1000,
            ttft_ms=0,
            output_tokens=0,
            error=str(e),
        )


# ─────────────────────────────────────────────
# Worker (runs in its own process)
# ─────────────────────────────────────────────

def worker_process(
    worker_id: int,
    target: str,
    model: str,
    rps_per_worker: float,
    duration: int,
    max_tokens: int,
    timeout: int,
    result_queue: multiprocessing.Queue,
    prompts: List[str],
):
    """Each worker runs its own asyncio event loop."""
    asyncio.run(
        worker_async(
            worker_id, target, model, rps_per_worker,
            duration, max_tokens, timeout, result_queue, prompts
        )
    )


async def worker_async(
    worker_id: int,
    target: str,
    model: str,
    rps_per_worker: float,
    duration: int,
    max_tokens: int,
    timeout: int,
    result_queue: multiprocessing.Queue,
    prompts: List[str],
):
    interval = 1.0 / rps_per_worker   # seconds between each request
    stats = BenchmarkStats()
    stats.start_time = time.monotonic()
    end_time = stats.start_time + duration

    connector = aiohttp.TCPConnector(
        limit=0,           # no connection limit
        limit_per_host=0,
        force_close=False,
        enable_cleanup_closed=True,
    )

    pending_tasks = set()

    async with aiohttp.ClientSession(connector=connector) as session:
        while True:
            now = time.monotonic()
            if now >= end_time:
                break

            # Fire the request
            prompt = random.choice(prompts)
            task = asyncio.create_task(
                send_request(session, target, model, prompt, max_tokens, timeout)
            )
            pending_tasks.add(task)
            task.add_done_callback(pending_tasks.discard)

            # Collect any completed results
            done = {t for t in pending_tasks if t.done()}
            for t in done:
                result = t.result()
                stats.total_requests += 1
                if result.success:
                    stats.successful += 1
                    stats.latencies_ms.append(result.latency_ms)
                    stats.ttfts_ms.append(result.ttft_ms)
                    stats.output_tokens_list.append(result.output_tokens)
                else:
                    stats.failed += 1
                    stats.errors[result.error] += 1

            # Sleep to maintain target RPS
            elapsed = time.monotonic() - now
            sleep_time = interval - elapsed
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)

        # Wait for all pending tasks
        if pending_tasks:
            results = await asyncio.gather(*pending_tasks, return_exceptions=True)
            for result in results:
                if isinstance(result, RequestResult):
                    stats.total_requests += 1
                    if result.success:
                        stats.successful += 1
                        stats.latencies_ms.append(result.latency_ms)
                        stats.ttfts_ms.append(result.ttft_ms)
                        stats.output_tokens_list.append(result.output_tokens)
                    else:
                        stats.failed += 1
                        stats.errors[result.error] += 1

    stats.end_time = time.monotonic()
    result_queue.put(stats)


# ─────────────────────────────────────────────
# Get model name from server
# ─────────────────────────────────────────────

async def get_model_name(target: str) -> str:
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{target}/v1/models",
                timeout=aiohttp.ClientTimeout(total=10),
            ) as resp:
                data = await resp.json()
                return data["data"][0]["id"]
    except Exception:
        return "unknown-model"


# ─────────────────────────────────────────────
# Print Results
# ─────────────────────────────────────────────

def print_results(all_stats: List[BenchmarkStats], target_rps: int, duration: int, num_workers: int):
    # Merge all worker stats
    merged = BenchmarkStats()
    merged.start_time = min(s.start_time for s in all_stats)
    merged.end_time = max(s.end_time for s in all_stats)

    for s in all_stats:
        merged.total_requests += s.total_requests
        merged.successful += s.successful
        merged.failed += s.failed
        merged.latencies_ms.extend(s.latencies_ms)
        merged.ttfts_ms.extend(s.ttfts_ms)
        merged.output_tokens_list.extend(s.output_tokens_list)
        for k, v in s.errors.items():
            merged.errors[k] += v

    actual_duration = merged.end_time - merged.start_time
    actual_rps = merged.successful / actual_duration if actual_duration > 0 else 0
    total_output_tokens = sum(merged.output_tokens_list)
    output_tps = total_output_tokens / actual_duration if actual_duration > 0 else 0

    print("\n" + "═" * 60)
    print("         LLM BENCHMARK RESULTS")
    print("═" * 60)
    print(f"  Target RPS        : {target_rps}")
    print(f"  Workers           : {num_workers}")
    print(f"  Duration          : {actual_duration:.1f}s")
    print()
    print(f"  ── Requests ──────────────────────────────")
    print(f"  Total sent        : {merged.total_requests}")
    print(f"  Successful        : {merged.successful}")
    print(f"  Failed            : {merged.failed}")
    success_rate = (merged.successful / merged.total_requests * 100) if merged.total_requests > 0 else 0
    print(f"  Success rate      : {success_rate:.1f}%")
    print()
    print(f"  ── Actual Throughput ─────────────────────")
    print(f"  Actual RPS        : {actual_rps:.1f}")
    print(f"  Output tokens/s   : {output_tps:.1f}")
    print()

    if merged.ttfts_ms:
        print(f"  ── TTFT (Time to First Token) ────────────")
        print(f"  Mean              : {sum(merged.ttfts_ms)/len(merged.ttfts_ms):.1f}ms")
        print(f"  P50               : {merged.percentile(merged.ttfts_ms, 50):.1f}ms")
        print(f"  P90               : {merged.percentile(merged.ttfts_ms, 90):.1f}ms")
        print(f"  P95               : {merged.percentile(merged.ttfts_ms, 95):.1f}ms")
        print(f"  P99               : {merged.percentile(merged.ttfts_ms, 99):.1f}ms")
        print()

    if merged.latencies_ms:
        print(f"  ── E2E Latency ───────────────────────────")
        print(f"  Mean              : {sum(merged.latencies_ms)/len(merged.latencies_ms):.1f}ms")
        print(f"  P50               : {merged.percentile(merged.latencies_ms, 50):.1f}ms")
        print(f"  P90               : {merged.percentile(merged.latencies_ms, 90):.1f}ms")
        print(f"  P95               : {merged.percentile(merged.latencies_ms, 95):.1f}ms")
        print(f"  P99               : {merged.percentile(merged.latencies_ms, 99):.1f}ms")
        print()

    if merged.errors:
        print(f"  ── Errors ────────────────────────────────")
        for err, count in sorted(merged.errors.items(), key=lambda x: -x[1]):
            print(f"  {err:<30}: {count}")
        print()

    # Verdict
    gap = actual_rps / target_rps * 100 if target_rps > 0 else 0
    print(f"  ── Verdict ───────────────────────────────")
    if gap >= 95:
        print(f"  ✅ Server handled {target_rps} RPS  ({gap:.1f}% of target)")
    elif gap >= 70:
        print(f"  ⚠️  Server reached {actual_rps:.0f}/{target_rps} RPS  ({gap:.1f}% of target)")
        print(f"     → Server is approaching saturation")
    else:
        print(f"  ❌ Server only reached {actual_rps:.0f}/{target_rps} RPS  ({gap:.1f}% of target)")
        print(f"     → Server is saturated — add more replicas")
    print("═" * 60 + "\n")

    # Save JSON results
    output = {
        "config": {
            "target_rps": target_rps,
            "duration": duration,
            "workers": num_workers,
        },
        "results": {
            "actual_rps": round(actual_rps, 2),
            "total_requests": merged.total_requests,
            "successful": merged.successful,
            "failed": merged.failed,
            "success_rate_pct": round(success_rate, 2),
            "output_tokens_per_sec": round(output_tps, 2),
            "ttft_ms": {
                "mean": round(sum(merged.ttfts_ms)/len(merged.ttfts_ms), 2) if merged.ttfts_ms else 0,
                "p50": round(merged.percentile(merged.ttfts_ms, 50), 2),
                "p90": round(merged.percentile(merged.ttfts_ms, 90), 2),
                "p95": round(merged.percentile(merged.ttfts_ms, 95), 2),
                "p99": round(merged.percentile(merged.ttfts_ms, 99), 2),
            },
            "e2e_latency_ms": {
                "mean": round(sum(merged.latencies_ms)/len(merged.latencies_ms), 2) if merged.latencies_ms else 0,
                "p50": round(merged.percentile(merged.latencies_ms, 50), 2),
                "p90": round(merged.percentile(merged.latencies_ms, 90), 2),
                "p95": round(merged.percentile(merged.latencies_ms, 95), 2),
                "p99": round(merged.percentile(merged.latencies_ms, 99), 2),
            },
            "errors": dict(merged.errors),
        }
    }

    output_file = f"benchmark_results_{target_rps}rps.json"
    with open(output_file, "w") as f:
        json.dump(output, f, indent=2)
    print(f"  Full results saved → {output_file}\n")


# ─────────────────────────────────────────────
# Progress Monitor
# ─────────────────────────────────────────────

async def progress_monitor(duration: int):
    start = time.monotonic()
    while True:
        elapsed = time.monotonic() - start
        remaining = duration - elapsed
        if remaining <= 0:
            break
        bar_len = 30
        filled = int(bar_len * elapsed / duration)
        bar = "█" * filled + "░" * (bar_len - filled)
        print(f"\r  [{bar}] {elapsed:.0f}s / {duration}s", end="", flush=True)
        await asyncio.sleep(1)
    print()


# ─────────────────────────────────────────────
# Main
# ─────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(description="LLM RPS Benchmark Tool")
    parser.add_argument("--target",   default="http://localhost:8000", help="Server URL")
    parser.add_argument("--model",    default=None,  help="Model name (auto-detected if not set)")
    parser.add_argument("--rps",      type=int, default=100, help="Target requests per second")
    parser.add_argument("--duration", type=int, default=60,  help="Test duration in seconds")
    parser.add_argument("--workers",  type=int, default=None, help="Number of worker processes (default: auto)")
    parser.add_argument("--max-tokens", type=int, default=50, help="Max output tokens per request")
    parser.add_argument("--timeout",  type=int, default=120, help="Request timeout in seconds")
    parser.add_argument("--prompts-file", default=None, help="Path to JSONL file with prompts")
    args = parser.parse_args()

    # Auto-detect workers — 1 worker per ~700 RPS
    if args.workers is None:
        args.workers = max(1, (args.rps // 600) + 1)
        args.workers = min(args.workers, multiprocessing.cpu_count())

    # Auto-detect model
    if args.model is None:
        args.model = asyncio.run(get_model_name(args.target))

    # Load prompts
    prompts = SAMPLE_PROMPTS
    if args.prompts_file and os.path.exists(args.prompts_file):
        with open(args.prompts_file) as f:
            loaded = []
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    # Support {"prompt": "..."} or {"messages": [...]}
                    if "prompt" in obj:
                        loaded.append(obj["prompt"])
                    elif "messages" in obj:
                        content = obj["messages"][-1].get("content", "")
                        loaded.append(content)
                except json.JSONDecodeError:
                    loaded.append(line)
            if loaded:
                prompts = loaded
                print(f"  Loaded {len(prompts)} prompts from {args.prompts_file}")

    rps_per_worker = args.rps / args.workers

    print(f"\n{'═'*60}")
    print(f"  LLM RPS BENCHMARK")
    print(f"{'═'*60}")
    print(f"  Target     : {args.target}")
    print(f"  Model      : {args.model}")
    print(f"  Target RPS : {args.rps}")
    print(f"  Workers    : {args.workers} × {rps_per_worker:.0f} RPS each")
    print(f"  Duration   : {args.duration}s")
    print(f"  Max tokens : {args.max_tokens}")
    print(f"  Prompts    : {len(prompts)}")
    print(f"{'═'*60}\n")
    print("  Running benchmark...\n")

    result_queue = multiprocessing.Queue()

    # Spawn worker processes
    processes = []
    for i in range(args.workers):
        p = multiprocessing.Process(
            target=worker_process,
            args=(
                i, args.target, args.model,
                rps_per_worker, args.duration,
                args.max_tokens, args.timeout,
                result_queue, prompts,
            ),
        )
        p.start()
        processes.append(p)

    # Show progress bar in main process
    asyncio.run(progress_monitor(args.duration))

    # Wait for all workers
    for p in processes:
        p.join()

    # Collect results
    all_stats = []
    while not result_queue.empty():
        all_stats.append(result_queue.get())

    if not all_stats:
        print("  ❌ No results collected — check if the server is reachable")
        return

    print_results(all_stats, args.rps, args.duration, args.workers)


if __name__ == "__main__":
    main()
