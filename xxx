import yaml
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, asdict
from jinja2 import Template, Environment, BaseLoader
from kubernetes import client
from kserve import KServeClient
from kserve import V1beta1InferenceService
from kserve import V1beta1InferenceServiceSpec
from kserve import V1beta1PredictorSpec
from enum import Enum
import argparse
from datetime import datetime
import jsonschema

class DeploymentMode(Enum):
    KNATIVE = "Knative"
    RAW_DEPLOYMENT = "RawDeployment"

class Framework(Enum):
    TRITON = "triton"

class RuntimeType(Enum):
    BUILTIN = "builtin"  # Use built-in serving runtime
    CLUSTER = "cluster"  # Use ClusterServingRuntime
    CUSTOM = "custom"    # Use custom container

@dataclass
class ResourceConfig:
    cpu_request: str = "100m"
    memory_request: str = "512Mi"
    cpu_limit: str = "1"
    memory_limit: str = "2Gi"
    gpu_limit: Optional[str] = None

@dataclass
class ScalingConfig:
    min_replicas: int = 1
    max_replicas: int = 3
    scale_target: Optional[int] = None
    scale_metric: str = "concurrency"

@dataclass
class ClusterServingRuntimeConfig:
    """Configuration for ClusterServingRuntime usage"""
    runtime_name: str
    runtime_version: Optional[str] = None
    protocol_version: str = "v1"
    model_format: Optional[str] = None
    
@dataclass
class InferenceServiceConfig:
    name: str
    storage_uri: str  # Now required as input
    namespace: str = "default"
    framework: Framework = Framework.TRITON
    deployment_mode: DeploymentMode = DeploymentMode.KNATIVE
    runtime_type: RuntimeType = RuntimeType.CLUSTER
    cluster_runtime: Optional[ClusterServingRuntimeConfig] = None
    protocol_version: str = "v1"
    custom_image: Optional[str] = None
    env_vars: Optional[Dict[str, str]] = None
    resources: ResourceConfig = None
    scaling: ScalingConfig = None
    annotations: Optional[Dict[str, str]] = None
    labels: Optional[Dict[str, str]] = None
    service_account: Optional[str] = None
    node_selector: Optional[Dict[str, str]] = None
    tolerations: Optional[List[Dict[str, Any]]] = None
    canary_traffic_percent: Optional[int] = None
    transformer_config: Optional[Dict[str, Any]] = None
    explainer_config: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.resources is None:
            self.resources = ResourceConfig()
        if self.scaling is None:
            self.scaling = ScalingConfig()
        if isinstance(self.framework, str):
            self.framework = Framework(self.framework)
        if isinstance(self.deployment_mode, str):
            self.deployment_mode = DeploymentMode(self.deployment_mode)
        if isinstance(self.runtime_type, str):
            self.runtime_type = RuntimeType(self.runtime_type)
        
        # Set default cluster runtime if using cluster runtime type
        if self.runtime_type == RuntimeType.CLUSTER and self.cluster_runtime is None:
            self.cluster_runtime = ClusterServingRuntimeConfig(
                runtime_name="triton-server",
                protocol_version=self.protocol_version
            )

class ConfigLoader:
    """Configuration file loader with validation"""
    
    # JSON Schema for configuration validation
    CONFIG_SCHEMA = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "type": "object",
        "required": ["metadata", "storage", "runtime"],
        "properties": {
            "metadata": {
                "type": "object",
                "required": ["name"],
                "properties": {
                    "name": {
                        "type": "string",
                        "pattern": "^[a-z0-9]([-a-z0-9]*[a-z0-9])?$"
                    },
                    "namespace": {
                        "type": "string",
                        "default": "default"
                    }
                }
            },
            "storage": {
                "type": "object",
                "required": ["uri"],
                "properties": {
                    "uri": {
                        "type": "string"
                    }
                }
            },
            "runtime": {
                "type": "object",
                "properties": {
                    "framework": {
                        "type": "string",
                        "enum": ["triton"]
                    },
                    "type": {
                        "type": "string",
                        "enum": ["builtin", "cluster", "custom"]
                    },
                    "deployment_mode": {
                        "type": "string",
                        "enum": ["Knative", "RawDeployment"]
                    }
                }
            }
        }
    }
    
    @staticmethod
    def load_config_file(config_path: Union[str, Path]) -> Dict[str, Any]:
        """Load configuration from YAML or JSON file"""
        config_path = Path(config_path)
        
        if not config_path.exists():
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        
        with open(config_path, 'r') as f:
            if config_path.suffix.lower() in ['.yaml', '.yml']:
                config_dict = yaml.safe_load(f)
            elif config_path.suffix.lower() == '.json':
                config_dict = json.load(f)
            else:
                raise ValueError(f"Unsupported file format: {config_path.suffix}")
        
        return config_dict or {}
    
    @staticmethod
    def validate_config(config_dict: Dict[str, Any]) -> None:
        """Validate configuration against schema"""
        try:
            jsonschema.validate(config_dict, ConfigLoader.CONFIG_SCHEMA)
        except jsonschema.ValidationError as e:
            raise ValueError(f"Configuration validation failed: {e.message}")
    
    @staticmethod
    def config_to_inference_service(config_dict: Dict[str, Any]) -> InferenceServiceConfig:
        """Convert configuration dictionary to InferenceServiceConfig"""
        
        # Validate configuration
        ConfigLoader.validate_config(config_dict)
        
        # Extract configuration sections
        metadata = config_dict.get('metadata', {})
        storage = config_dict.get('storage', {})
        runtime = config_dict.get('runtime', {})
        resources_dict = config_dict.get('resources', {})
        scaling_dict = config_dict.get('scaling', {})
        kubernetes = config_dict.get('kubernetes', {})
        advanced = config_dict.get('advanced', {})
        
        # Create resource configuration
        resources = ResourceConfig(
            cpu_request=resources_dict.get('cpu_request', '100m'),
            memory_request=resources_dict.get('memory_request', '512Mi'),
            cpu_limit=resources_dict.get('cpu_limit', '1'),
            memory_limit=resources_dict.get('memory_limit', '2Gi'),
            gpu_limit=resources_dict.get('gpu_limit')
        )
        
        # Create scaling configuration
        scaling = ScalingConfig(
            min_replicas=scaling_dict.get('min_replicas', 1),
            max_replicas=scaling_dict.get('max_replicas', 3),
            scale_target=scaling_dict.get('scale_target'),
            scale_metric=scaling_dict.get('scale_metric', 'concurrency')
        )
        
        # Create cluster runtime configuration if specified
        cluster_runtime = None
        if runtime.get('type') == 'cluster' and 'cluster_runtime' in runtime:
            cr = runtime['cluster_runtime']
            cluster_runtime = ClusterServingRuntimeConfig(
                runtime_name=cr.get('runtime_name', 'triton-server'),
                runtime_version=cr.get('runtime_version'),
                protocol_version=cr.get('protocol_version', 'v1'),
                model_format=cr.get('model_format')
            )
        
        # Create main configuration
        inference_config = InferenceServiceConfig(
            name=metadata.get('name', 'default-service'),
            storage_uri=storage.get('uri', ''),
            namespace=metadata.get('namespace', 'default'),
            framework=Framework(runtime.get('framework', 'triton')),
            deployment_mode=DeploymentMode(runtime.get('deployment_mode', 'Knative')),
            runtime_type=RuntimeType(runtime.get('type', 'cluster')),
            cluster_runtime=cluster_runtime,
            protocol_version=runtime.get('protocol_version', 'v1'),
            custom_image=runtime.get('custom_image'),
            env_vars=config_dict.get('environment'),
            resources=resources,
            scaling=scaling,
            annotations=config_dict.get('annotations'),
            labels=config_dict.get('labels'),
            service_account=kubernetes.get('service_account'),
            node_selector=kubernetes.get('node_selector'),
            tolerations=kubernetes.get('tolerations'),
            canary_traffic_percent=advanced.get('canary_traffic_percent'),
            transformer_config=advanced.get('transformer'),
            explainer_config=advanced.get('explainer')
        )
        
        return inference_config

class KServeTemplatingClient:
    def __init__(self, namespace: str = "default", output_dir: Optional[str] = None):
        """
        Initialize KServe Templating Client
        
        Args:
            namespace: Default Kubernetes namespace
            output_dir: Default output directory for saving templates
        """
        self.namespace = namespace
        self.output_dir = Path(output_dir) if output_dir else Path.cwd() / "kserve-templates"
        self.kserve_client = None
        self._init_templates()
        
        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _init_templates(self):
        """Initialize Jinja2 templates for different components"""
        self.templates = {
            'inference_service': """
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ config.name }}
  namespace: {{ config.namespace }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
"""
        }
        
        # Initialize Jinja2 environment
        self.jinja_env = Environment(loader=BaseLoader())
        self.jinja_env.filters['tojson'] = json.dumps
    
    def _render_framework_spec(self, config: InferenceServiceConfig) -> str:
        """Render framework-specific specification"""
        # Use ClusterServingRuntime when specified
        if config.runtime_type == RuntimeType.CLUSTER:
            template = self.jinja_env.from_string(self.templates['cluster_runtime_spec'])
            model_format = None
            if config.cluster_runtime and hasattr(config.cluster_runtime, 'model_format'):
                model_format = config.cluster_runtime.model_format
            return template.render(
                config=config,
                env_vars=config.env_vars,
                resources=config.resources is not None,
                model_format=model_format
            )
        elif config.runtime_type == RuntimeType.CUSTOM:
            template = self.jinja_env.from_string(self.templates['custom_spec'])
            return template.render(
                config=config,
                env_vars=config.env_vars,
                resources=config.resources is not None
            )
        
        # Use built-in serving runtime
        template_key = f"{config.framework.value}_spec"
        
        if template_key not in self.templates:
            raise ValueError(f"Unsupported framework: {config.framework.value}")
        
        template = self.jinja_env.from_string(self.templates[template_key])
        return template.render(
            config=config,
            env_vars=config.env_vars,
            resources=config.resources is not None
        )
    
    def _render_transformer_spec(self, transformer_config: Dict[str, Any]) -> str:
        """Render transformer specification"""
        if not transformer_config:
            return ""
        
        # Simple transformer template
        template_str = """
containers:
- name: transformer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**transformer_config)
    
    def _render_explainer_spec(self, explainer_config: Dict[str, Any]) -> str:
        """Render explainer specification"""
        if not explainer_config:
            return ""
        
        # Simple explainer template
        template_str = """
containers:
- name: explainer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**explainer_config)
    
    def render_inference_service_yaml(self, config: InferenceServiceConfig) -> str:
        """
        Render InferenceService as YAML string
        
        Args:
            config: InferenceService configuration
            
        Returns:
            str: Rendered YAML string
        """
        # Prepare annotations
        annotations = config.annotations or {}
        if config.deployment_mode == DeploymentMode.RAW_DEPLOYMENT:
            annotations["serving.kserve.io/deploymentMode"] = "RawDeployment"
        
        # Render framework specification
        framework_spec = self._render_framework_spec(config)
        
        # Render transformer and explainer specs if provided
        transformer_spec = self._render_transformer_spec(config.transformer_config) if config.transformer_config else ""
        explainer_spec = self._render_explainer_spec(config.explainer_config) if config.explainer_config else ""
        
        # Render main template
        template = self.jinja_env.from_string(self.templates['inference_service'])
        rendered = template.render(
            config=config,
            annotations=annotations if annotations else None,
            labels=config.labels,
            framework_spec=framework_spec,
            transformer_spec=transformer_spec if transformer_spec else None,
            explainer_spec=explainer_spec if explainer_spec else None
        )
        
        return rendered.strip()
    
    def render_inference_service_dict(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Render InferenceService as Python dictionary
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Rendered InferenceService as dictionary
        """
        yaml_str = self.render_inference_service_yaml(config)
        return yaml.safe_load(yaml_str)
    
    def load_from_config_file(self, config_path: Union[str, Path]) -> InferenceServiceConfig:
        """
        Load configuration from file and create InferenceServiceConfig
        
        Args:
            config_path: Path to configuration file (YAML or JSON)
            
        Returns:
            InferenceServiceConfig: Parsed configuration
        """
        config_dict = ConfigLoader.load_config_file(config_path)
        return ConfigLoader.config_to_inference_service(config_dict)
    
    def save_template_to_file(self, config: InferenceServiceConfig, filename: Optional[str] = None, output_dir: Optional[str] = None) -> str:
        """
        Save rendered template to file
        
        Args:
            config: InferenceService configuration
            filename: Custom filename (optional)
            output_dir: Custom output directory (optional)
            
        Returns:
            str: Path to saved file
        """
        # Determine output directory
        save_dir = Path(output_dir) if output_dir else self.output_dir
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate filename if not provided
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{config.name}_{config.framework.value}_{timestamp}.yaml"
        
        # Ensure .yaml extension
        if not filename.endswith(('.yaml', '.yml')):
            filename += '.yaml'
        
        filepath = save_dir / filename
        
        # Render and save
        yaml_content = self.render_inference_service_yaml(config)
        with open(filepath, 'w') as f:
            f.write(yaml_content)
        
        print(f"Template saved to: {filepath}")
        return str(filepath)
    
    def generate_sample_config(self, config_type: str = "basic", output_path: Optional[str] = None) -> str:
        """
        Generate sample configuration files
        
        Args:
            config_type: Type of config ('basic', 'gpu', 'multi-model', 'complete')
            output_path: Path to save the config file
            
        Returns:
            str: Path to generated config file
        """
        configs = {
            "basic": {
                "metadata": {
                    "name": "simple-model",
                    "namespace": "default"
                },
                "storage": {
                    "uri": "s3://my-bucket/models/simple-model/"
                },
                "runtime": {
                    "framework": "triton",
                    "type": "cluster",
                    "cluster_runtime": {
                        "runtime_name": "triton-server"
                    }
                },
                "resources": {
                    "cpu_request": "100m",
                    "memory_request": "512Mi",
                    "cpu_limit": "1",
                    "memory_limit": "2Gi"
                },
                "scaling": {
                    "min_replicas": 1,
                    "max_replicas": 3
                }
            },
            "gpu": {
                "metadata": {
                    "name": "gpu-model",
                    "namespace": "ml-inference"
                },
                "storage": {
                    "uri": "s3://ai-models/bert-large/v1.0/"
                },
                "runtime": {
                    "framework": "triton",
                    "type": "cluster",
                    "deployment_mode": "RawDeployment",
                    "cluster_runtime": {
                        "runtime_name": "triton-server-gpu",
                        "runtime_version": "2.29.0",
                        "model_format": "pytorch_libtorch"
                    }
                },
                "resources": {
                    "cpu_request": "2",
                    "memory_request": "8Gi",
                    "cpu_limit": "4",
                    "memory_limit": "16Gi",
                    "gpu_limit": "1"
                },
                "scaling": {
                    "min_replicas": 1,
                    "max_replicas": 3,
                    "scale_target": 70
                },
                "environment": {
                    "CUDA_VISIBLE_DEVICES": "0",
                    "TRITON_LOG_VERBOSE": "1"
                },
                "kubernetes": {
                    "node_selector": {
                        "accelerator": "nvidia-tesla-v100"
                    },
                    "tolerations": [
                        {
                            "key": "nvidia.com/gpu",
                            "operator": "Equal",
                            "value": "present",
                            "effect": "NoSchedule"
                        }
                    ]
                },
                "annotations": {
                    "serving.kserve.io/deploymentMode": "RawDeployment"
                }
            },
            "complete": {
                "metadata": {
                    "name": "advanced-model",
                    "namespace": "production"
                },
                "storage": {
                    "uri": "s3://ml-models/advanced-model/v2.0/"
                },
                "runtime": {
                    "framework": "triton",
                    "type": "cluster",
                    "deployment_mode": "Knative",
                    "protocol_version": "v1",
                    "cluster_runtime": {
                        "runtime_name": "triton-server",
                        "runtime_version": "2.29.0",
                        "model_format": "ensemble",
                        "protocol_version": "v1"
                    }
                },
                "resources": {
                    "cpu_request": "1",
                    "memory_request": "2Gi",
                    "cpu_limit": "4",
                    "memory_limit": "8Gi",
                    "gpu_limit": "1"
                },
                "scaling": {
                    "min_replicas": 2,
                    "max_replicas": 10,
                    "scale_target": 80,
                    "scale_metric": "concurrency"
                },
                "environment": {
                    "MODEL_NAME": "advanced-model",
                    "MAX_BATCH_SIZE": "32",
                    "TRITON_LOG_VERBOSE": "1"
                },
                "kubernetes": {
                    "service_account": "kserve-sa",
                    "node_selector": {
                        "workload-type": "ml-inference"
                    }
                },
                "annotations": {
                    "serving.kserve.io/enable-prometheus-scraping": "true",
                    "autoscaling.knative.dev/target": "80"
                },
                "labels": {
                    "app": "advanced-model",
                    "version": "v2.0",
                    "team": "ml-platform"
                },
                "advanced": {
                    "canary_traffic_percent": 10,
                    "transformer": {
                        "image": "my-registry/text-transformer:v1.0",
                        "env": {
                            "TOKENIZER_PATH": "/models/tokenizer",
                            "MAX_SEQUENCE_LENGTH": "2048"
                        },
                        "resources": {
                            "requests": {
                                "cpu": "500m",
                                "memory": "1Gi"
                            },
                            "limits": {
                                "cpu": "1",
                                "memory": "2Gi"
                            }
                        }
                    }
                }
            }
        }
        
        if config_type not in configs:
            raise ValueError(f"Unknown config type: {config_type}. Available: {list(configs.keys())}")
        
        config_data = configs[config_type]
        
        # Determine output path
        if not output_path:
            output_path = f"kserve-config-{config_type}.yaml"
        
        # Save config
        with open(output_path, 'w') as f:
            yaml.dump(config_data, f, default_flow_style=False, indent=2)
        
        print(f"Sample {config_type} configuration saved to: {output_path}")
        return output_path
    
    def validate_template(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Validate the rendered template
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Validation result with status and any errors
        """
        try:
            # Try to render and parse
            yaml_str = self.render_inference_service_yaml(config)
            parsed = yaml.safe_load(yaml_str)
            
            # Basic validation
            errors = []
            if not parsed.get('metadata', {}).get('name'):
                errors.append("Missing metadata.name")
            if not parsed.get('spec', {}).get('predictor'):
                errors.append("Missing spec.predictor")
            if not config.storage_uri:
                errors.append("Missing storage_uri")
            
            return {
                "valid": len(errors) == 0,
                "errors": errors,
                "template": parsed
            }
        except Exception as e:
            return {
                "valid": False,
                "errors": [str(e)],
                "template": None
            }

# Helper functions
def create_cluster_runtime_config(name: str, storage_uri: str, runtime_name: str = "triton-server", **kwargs) -> InferenceServiceConfig:
    """Helper to create ClusterServingRuntime configuration"""
    cluster_runtime = ClusterServingRuntimeConfig(
        runtime_name=runtime_name,
        **{k: v for k, v in kwargs.items() if k in ['runtime_version', 'protocol_version', 'model_format']}
    )
    
    return InferenceServiceConfig(
        name=name,
        storage_uri=storage_uri,
        framework=Framework.TRITON,
        runtime_type=RuntimeType.CLUSTER,
        cluster_runtime=cluster_runtime,
        **{k: v for k, v in kwargs.items() if k not in ['runtime_version', 'protocol_version', 'model_format']}
    )

def create_triton_config(name: str, storage_uri: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create triton configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.TRITON,
        storage_uri=storage_uri,
        **kwargs
    )

def create_custom_config(name: str, storage_uri: str, custom_image: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create custom configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.TRITON,
        runtime_type=RuntimeType.CUSTOM,
        storage_uri=storage_uri,
        custom_image=custom_image,
        **kwargs
    )

def main():
    """CLI entry point with config file support"""
    parser = argparse.ArgumentParser(description="KServe Triton Template Generator")
    
    # Config file option
    parser.add_argument("--config", help="Path to configuration file (YAML or JSON)")
    parser.add_argument("--generate-config", choices=["basic", "gpu", "complete"], 
                       help="Generate sample configuration file")
    
    # Individual options (can override config file)
    parser.add_argument("--name", help="InferenceService name")
    parser.add_argument("--storage-uri", help="Model storage URI")
    parser.add_argument("--runtime-type", choices=[r.value for r in RuntimeType],
                       default="cluster", help="Runtime type")
    parser.add_argument("--runtime-name", default="triton-server", help="ClusterServingRuntime name")
    parser.add_argument("--namespace", default="default", help="Kubernetes namespace")
    parser.add_argument("--output-dir", help="Output directory for templates")
    parser.add_argument("--custom-image", help="Custom container image")
    parser.add_argument("--cpu-request", default="100m", help="CPU request")
    parser.add_argument("--memory-request", default="512Mi", help="Memory request")
    parser.add_argument("--cpu-limit", default="1", help="CPU limit")
    parser.add_argument("--memory-limit", default="2Gi", help="Memory limit")
    parser.add_argument("--gpu-limit", help="GPU limit")
    parser.add_argument("--min-replicas", type=int, default=1, help="Minimum replicas")
    parser.add_argument("--max-replicas", type=int, default=3, help="Maximum replicas")
    parser.add_argument("--deployment-mode", choices=[d.value for d in DeploymentMode],
                       default="Knative", help="Deployment mode")
    parser.add_argument("--validate-only", action="store_true", help="Only validate, don't generate")
    
    args = parser.parse_args()
    
    # Initialize client
    client = KServeTemplatingClient(
        namespace=args.namespace,
        output_dir=args.output_dir
    )
    
    # Generate sample config if requested
    if args.generate_config:
        client.generate_sample_config(args.generate_config)
        return
    
    # Load from config file or create from CLI args
    if args.config:
        print(f"Loading configuration from: {args.config}")
        try:
            config = client.load_from_config_file(args.config)
            
            # Override with CLI arguments if provided
            if args.name:
                config.name = args.name
            if args.storage_uri:
                config.storage_uri = args.storage_uri
            if args.namespace:
                config.namespace = args.namespace
            if args.min_replicas:
                config.scaling.min_replicas = args.min_replicas
            if args.max_replicas:
                config.scaling.max_replicas = args.max_replicas
                
        except Exception as e:
            print(f"❌ Error loading config file: {e}")
            return
    else:
        # Create configuration from CLI arguments
        if not args.name or not args.storage_uri:
            print("❌ --name and --storage-uri are required when not using config file")
            return
            
        if args.runtime_type == "cluster":
            config = create_cluster_runtime_config(
                name=args.name,
                storage_uri=args.storage_uri,
                runtime_name=args.runtime_name,
                namespace=args.namespace,
                deployment_mode=DeploymentMode(args.deployment_mode),
                resources=ResourceConfig(
                    cpu_request=args.cpu_request,
                    memory_request=args.memory_request,
                    cpu_limit=args.cpu_limit,
                    memory_limit=args.memory_limit,
                    gpu_limit=args.gpu_limit
                ),
                scaling=ScalingConfig(
                    min_replicas=args.min_replicas,
                    max_replicas=args.max_replicas
                )
            )
        elif args.runtime_type == "custom":
            if not args.custom_image:
                print("❌ --custom-image is required for custom runtime type")
                return
            config = create_custom_config(
                name=args.name,
                storage_uri=args.storage_uri,
                custom_image=args.custom_image,
                namespace=args.namespace,
                deployment_mode=DeploymentMode(args.deployment_mode),
                resources=ResourceConfig(
                    cpu_request=args.cpu_request,
                    memory_request=args.memory_request,
                    cpu_limit=args.cpu_limit,
                    memory_limit=args.memory_limit,
                    gpu_limit=args.gpu_limit
                ),
                scaling=ScalingConfig(
                    min_replicas=args.min_replicas,
                    max_replicas=args.max_replicas
                )
            )
        else:  # builtin
            config = InferenceServiceConfig(
                name=args.name,
                storage_uri=args.storage_uri,
                framework=Framework.TRITON,
                runtime_type=RuntimeType.BUILTIN,
                namespace=args.namespace,
                deployment_mode=DeploymentMode(args.deployment_mode),
                resources=ResourceConfig(
                    cpu_request=args.cpu_request,
                    memory_request=args.memory_request,
                    cpu_limit=args.cpu_limit,
                    memory_limit=args.memory_limit,
                    gpu_limit=args.gpu_limit
                ),
                scaling=ScalingConfig(
                    min_replicas=args.min_replicas,
                    max_replicas=args.max_replicas
                )
            )
    
    # Validate template
    validation = client.validate_template(config)
    if validation['valid']:
        print("✅ Template validation passed")
    else:
        print("❌ Template validation failed:")
        for error in validation['errors']:
            print(f"  - {error}")
        if args.validate_only:
            return
    
    # Generate and save template (unless validate-only)
    if not args.validate_only:
        try:
            filepath = client.save_template_to_file(config)
            print(f"✅ Template generated successfully: {filepath}")
            
            # Show preview of generated YAML
            print("\n--- Generated YAML Preview ---")
            yaml_content = client.render_inference_service_yaml(config)
            print(yaml_content[:500] + "..." if len(yaml_content) > 500 else yaml_content)
            
        except Exception as e:
            print(f"❌ Error generating template: {e}")

if __name__ == "__main__":
    main() if annotations %}
  annotations:
    {% for key, value in annotations.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if labels %}
  labels:
    {% for key, value in labels.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
spec:
  {% if config.service_account %}
  serviceAccountName: {{ config.service_account }}
  {% endif %}
  predictor:
    {% if config.scaling.min_replicas %}
    minReplicas: {{ config.scaling.min_replicas }}
    {% endif %}
    {% if config.scaling.max_replicas %}
    maxReplicas: {{ config.scaling.max_replicas }}
    {% endif %}
    {% if config.scaling.scale_target %}
    scaleTarget: {{ config.scaling.scale_target }}
    {% endif %}
    {% if config.scaling.scale_metric %}
    scaleMetric: {{ config.scaling.scale_metric }}
    {% endif %}
    {% if config.node_selector %}
    nodeSelector:
      {% for key, value in config.node_selector.items() %}
      {{ key }}: "{{ value }}"
      {% endfor %}
    {% endif %}
    {% if config.tolerations %}
    tolerations:
      {% for toleration in config.tolerations %}
      - {{ toleration | tojson }}
      {% endfor %}
    {% endif %}
    {{ framework_spec | indent(4, true) }}
  {% if transformer_spec %}
  transformer:
    {{ transformer_spec | indent(4, true) }}
  {% endif %}
  {% if explainer_spec %}
  explainer:
    {{ explainer_spec | indent(4, true) }}
  {% endif %}
""",
            'cluster_runtime_spec': """
model:
  modelFormat:
    name: {{ model_format or config.framework.value }}
    {% if config.cluster_runtime.runtime_version %}
    version: {{ config.cluster_runtime.runtime_version }}
    {% endif %}
  runtime: {{ config.cluster_runtime.runtime_name }}
  storageUri: {{ config.storage_uri }}
  {% if config.protocol_version %}
  protocolVersion: {{ config.protocol_version }}
  {% endif %}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'triton_spec': """
triton:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'custom_spec': """
containers:
- name: kserve-container
  image: {{ config.custom_image }}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
    {% if config.storage_uri %}
    - name: STORAGE_URI
      value: {{ config.storage_uri }}
    {% endif %}
  {% endif %}
  {%
