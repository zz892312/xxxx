# Set your project first
runai project set <your-project-name>

# Submit an inference workload with ONNX Runtime
runai submit onnx-inference \
  --image mcr.microsoft.com/onnxruntime/server:latest \
  --gpu 1 \
  --service-type nodeport \
  --port 8001 \
  --volume /path/to/your/model:/models \
  --env MODEL_PATH=/models/your-model.onnx \
  --command -- python -c "
import onnxruntime as ort
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import numpy as np

class ONNXHandler(BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.session = ort.InferenceSession('/models/your-model.onnx')
        super().__init__(*args, **kwargs)
    
    def do_POST(self):
        # Handle inference requests
        pass

httpd = HTTPServer(('0.0.0.0', 8001), ONNXHandler)
httpd.serve_forever()
"
