import yaml
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, asdict
from jinja2 import Template, Environment, BaseLoader
from kubernetes import client
from kserve import KServeClient
from kserve import V1beta1InferenceService
from kserve import V1beta1InferenceServiceSpec
from kserve import V1beta1PredictorSpec
from kserve import V1beta1SKLearnSpec
from kserve import V1beta1TensorflowSpec
from kserve import V1beta1PyTorchSpec
from kserve import V1beta1XGBoostSpec
from kserve import V1beta1LightGBMSpec
from enum import Enum
import argparse
from datetime import datetime

class DeploymentMode(Enum):
    KNATIVE = "Knative"
    RAW_DEPLOYMENT = "RawDeployment"

class Framework(Enum):
    SKLEARN = "sklearn"
    TENSORFLOW = "tensorflow"
    PYTORCH = "pytorch"
    XGBOOST = "xgboost"
    LIGHTGBM = "lightgbm"
    CUSTOM = "custom"
    HUGGINGFACE = "huggingface"

class RuntimeType(Enum):
    BUILTIN = "builtin"  # Use built-in serving runtime
    CLUSTER = "cluster"  # Use ClusterServingRuntime
    CUSTOM = "custom"    # Use custom container

@dataclass
class ResourceConfig:
    cpu_request: str = "100m"
    memory_request: str = "512Mi"
    cpu_limit: str = "1"
    memory_limit: str = "2Gi"
    gpu_limit: Optional[str] = None

@dataclass
class ScalingConfig:
    min_replicas: int = 1
    max_replicas: int = 3
    scale_target: Optional[int] = None
    scale_metric: str = "concurrency"

@dataclass
class ClusterServingRuntimeConfig:
    """Configuration for ClusterServingRuntime usage"""
    runtime_name: str
    runtime_version: Optional[str] = None
    protocol_version: str = "v1"
    model_format: Optional[str] = None
    
@dataclass
class InferenceServiceConfig:
    name: str
    storage_uri: str  # Now required as input
    namespace: str = "default"
    framework: Framework = Framework.SKLEARN
    deployment_mode: DeploymentMode = DeploymentMode.KNATIVE
    runtime_type: RuntimeType = RuntimeType.CLUSTER
    cluster_runtime: Optional[ClusterServingRuntimeConfig] = None
    protocol_version: str = "v1"
    custom_image: Optional[str] = None
    env_vars: Optional[Dict[str, str]] = None
    resources: ResourceConfig = None
    scaling: ScalingConfig = None
    annotations: Optional[Dict[str, str]] = None
    labels: Optional[Dict[str, str]] = None
    service_account: Optional[str] = None
    node_selector: Optional[Dict[str, str]] = None
    tolerations: Optional[List[Dict[str, Any]]] = None
    canary_traffic_percent: Optional[int] = None
    transformer_config: Optional[Dict[str, Any]] = None
    explainer_config: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.resources is None:
            self.resources = ResourceConfig()
        if self.scaling is None:
            self.scaling = ScalingConfig()
        if isinstance(self.framework, str):
            self.framework = Framework(self.framework)
        if isinstance(self.deployment_mode, str):
            self.deployment_mode = DeploymentMode(self.deployment_mode)
        if isinstance(self.runtime_type, str):
            self.runtime_type = RuntimeType(self.runtime_type)
        
        # Set default cluster runtime if using cluster runtime type
        if self.runtime_type == RuntimeType.CLUSTER and self.cluster_runtime is None:
            runtime_name_map = {
                Framework.SKLEARN: "sklearn-server",
                Framework.TENSORFLOW: "tensorflow-serving",
                Framework.PYTORCH: "pytorch-server",
                Framework.XGBOOST: "xgboost-server",
                Framework.LIGHTGBM: "lightgbm-server",
                Framework.HUGGINGFACE: "huggingface-server"
            }
            self.cluster_runtime = ClusterServingRuntimeConfig(
                runtime_name=runtime_name_map.get(self.framework, f"{self.framework.value}-server"),
                protocol_version=self.protocol_version
            )

class KServeTemplatingClient:
    def __init__(self, namespace: str = "default", output_dir: Optional[str] = None):
        """
        Initialize KServe Templating Client
        
        Args:
            namespace: Default Kubernetes namespace
            output_dir: Default output directory for saving templates
        """
        self.namespace = namespace
        self.output_dir = Path(output_dir) if output_dir else Path.cwd() / "kserve-templates"
        self.kserve_client = None
        self._init_templates()
        
        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _init_templates(self):
        """Initialize Jinja2 templates for different components"""
        self.templates = {
            'inference_service': """
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ config.name }}
  namespace: {{ config.namespace }}
  {% if annotations %}
  annotations:
    {% for key, value in annotations.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if labels %}
  labels:
    {% for key, value in labels.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
spec:
  {% if config.service_account %}
  serviceAccountName: {{ config.service_account }}
  {% endif %}
  predictor:
    {% if config.scaling.min_replicas %}
    minReplicas: {{ config.scaling.min_replicas }}
    {% endif %}
    {% if config.scaling.max_replicas %}
    maxReplicas: {{ config.scaling.max_replicas }}
    {% endif %}
    {% if config.scaling.scale_target %}
    scaleTarget: {{ config.scaling.scale_target }}
    {% endif %}
    {% if config.scaling.scale_metric %}
    scaleMetric: {{ config.scaling.scale_metric }}
    {% endif %}
    {% if config.node_selector %}
    nodeSelector:
      {% for key, value in config.node_selector.items() %}
      {{ key }}: "{{ value }}"
      {% endfor %}
    {% endif %}
    {% if config.tolerations %}
    tolerations:
      {% for toleration in config.tolerations %}
      - {{ toleration | tojson }}
      {% endfor %}
    {% endif %}
    {{ framework_spec | indent(4, true) }}
  {% if transformer_spec %}
  transformer:
    {{ transformer_spec | indent(4, true) }}
  {% endif %}
  {% if explainer_spec %}
  explainer:
    {{ explainer_spec | indent(4, true) }}
  {% endif %}
""",
            'cluster_runtime_spec': """
model:
  modelFormat:
    name: {{ model_format or config.framework.value }}
    {% if config.cluster_runtime.runtime_version %}
    version: {{ config.cluster_runtime.runtime_version }}
    {% endif %}
  runtime: {{ config.cluster_runtime.runtime_name }}
  storageUri: {{ config.storage_uri }}
  {% if config.protocol_version %}
  protocolVersion: {{ config.protocol_version }}
  {% endif %}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'sklearn_spec': """
sklearn:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'tensorflow_spec': """
tensorflow:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'pytorch_spec': """
pytorch:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'xgboost_spec': """
xgboost:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'lightgbm_spec': """
lightgbm:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'custom_spec': """
containers:
- name: kserve-container
  image: {{ config.custom_image }}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
    {% if config.storage_uri %}
    - name: STORAGE_URI
      value: {{ config.storage_uri }}
    {% endif %}
  {% endif %}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
""",
            'huggingface_spec': """
huggingface:
  storageUri: {{ config.storage_uri }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
"""
        }
        
        # Initialize Jinja2 environment
        self.jinja_env = Environment(loader=BaseLoader())
        self.jinja_env.filters['tojson'] = json.dumps
    
    def _render_framework_spec(self, config: InferenceServiceConfig) -> str:
        """Render framework-specific specification"""
        # Use ClusterServingRuntime when specified
        if config.runtime_type == RuntimeType.CLUSTER:
            template = self.jinja_env.from_string(self.templates['cluster_runtime_spec'])
            model_format = None
            if config.cluster_runtime and hasattr(config.cluster_runtime, 'model_format'):
                model_format = config.cluster_runtime.model_format
            return template.render(
                config=config,
                env_vars=config.env_vars,
                resources=config.resources is not None,
                model_format=model_format
            )
        
        # Use built-in serving runtime
        template_key = f"{config.framework.value}_spec"
        
        if template_key not in self.templates:
            raise ValueError(f"Unsupported framework: {config.framework.value}")
        
        template = self.jinja_env.from_string(self.templates[template_key])
        return template.render(
            config=config,
            env_vars=config.env_vars,
            resources=config.resources is not None
        )
    
    def _render_transformer_spec(self, transformer_config: Dict[str, Any]) -> str:
        """Render transformer specification"""
        if not transformer_config:
            return ""
        
        # Simple transformer template
        template_str = """
containers:
- name: transformer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**transformer_config)
    
    def _render_explainer_spec(self, explainer_config: Dict[str, Any]) -> str:
        """Render explainer specification"""
        if not explainer_config:
            return ""
        
        # Simple explainer template
        template_str = """
containers:
- name: explainer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**explainer_config)
    
    def render_inference_service_yaml(self, config: InferenceServiceConfig) -> str:
        """
        Render InferenceService as YAML string
        
        Args:
            config: InferenceService configuration
            
        Returns:
            str: Rendered YAML string
        """
        # Prepare annotations
        annotations = config.annotations or {}
        if config.deployment_mode == DeploymentMode.RAW_DEPLOYMENT:
            annotations["serving.kserve.io/deploymentMode"] = "RawDeployment"
        
        # Render framework specification
        framework_spec = self._render_framework_spec(config)
        
        # Render transformer and explainer specs if provided
        transformer_spec = self._render_transformer_spec(config.transformer_config) if config.transformer_config else ""
        explainer_spec = self._render_explainer_spec(config.explainer_config) if config.explainer_config else ""
        
        # Render main template
        template = self.jinja_env.from_string(self.templates['inference_service'])
        rendered = template.render(
            config=config,
            annotations=annotations if annotations else None,
            labels=config.labels,
            framework_spec=framework_spec,
            transformer_spec=transformer_spec if transformer_spec else None,
            explainer_spec=explainer_spec if explainer_spec else None
        )
        
        return rendered.strip()
    
    def render_inference_service_dict(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Render InferenceService as Python dictionary
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Rendered InferenceService as dictionary
        """
        yaml_str = self.render_inference_service_yaml(config)
        return yaml.safe_load(yaml_str)
    
    def save_template_to_file(self, config: InferenceServiceConfig, filename: Optional[str] = None, output_dir: Optional[str] = None) -> str:
        """
        Save rendered template to file
        
        Args:
            config: InferenceService configuration
            filename: Custom filename (optional)
            output_dir: Custom output directory (optional)
            
        Returns:
            str: Path to saved file
        """
        # Determine output directory
        save_dir = Path(output_dir) if output_dir else self.output_dir
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate filename if not provided
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{config.name}_{config.framework.value}_{timestamp}.yaml"
        
        # Ensure .yaml extension
        if not filename.endswith(('.yaml', '.yml')):
            filename += '.yaml'
        
        filepath = save_dir / filename
        
        # Render and save
        yaml_content = self.render_inference_service_yaml(config)
        with open(filepath, 'w') as f:
            f.write(yaml_content)
        
        print(f"Template saved to: {filepath}")
        return str(filepath)
    
    def save_batch_templates(self, configs: List[InferenceServiceConfig], output_dir: Optional[str] = None) -> List[str]:
        """
        Save multiple templates to files
        
        Args:
            configs: List of InferenceService configurations
            output_dir: Custom output directory (optional)
            
        Returns:
            List[str]: Paths to saved files
        """
        saved_files = []
        for config in configs:
            filepath = self.save_template_to_file(config, output_dir=output_dir)
            saved_files.append(filepath)
        return saved_files
    
    def create_inference_service_object(self, config: InferenceServiceConfig) -> V1beta1InferenceService:
        """
        Create KServe InferenceService object from configuration
        
        Args:
            config: InferenceService configuration
            
        Returns:
            V1beta1InferenceService: KServe InferenceService object
        """
        isvc_dict = self.render_inference_service_dict(config)
        
        # Convert dictionary to KServe object
        metadata = client.V1ObjectMeta(
            name=config.name,
            namespace=config.namespace,
        )
        
        # Add annotations if any
        annotations = config.annotations or {}
        if config.deployment_mode == DeploymentMode.RAW_DEPLOYMENT:
            annotations["serving.kserve.io/deploymentMode"] = "RawDeployment"
        if annotations:
            metadata.annotations = annotations
        
        # Add labels if any
        if config.labels:
            metadata.labels = config.labels
        
        # Create predictor spec based on framework and runtime type
        predictor_kwargs = {
            "min_replicas": config.scaling.min_replicas,
            "max_replicas": config.scaling.max_replicas,
        }
        
        # Add resources if specified
        resources = None
        if config.resources:
            resources = client.V1ResourceRequirements(
                requests={
                    "cpu": config.resources.cpu_request,
                    "memory": config.resources.memory_request
                },
                limits={
                    "cpu": config.resources.cpu_limit,
                    "memory": config.resources.memory_limit
                }
            )
            if config.resources.gpu_limit:
                resources.limits["nvidia.com/gpu"] = config.resources.gpu_limit
        
        # Handle different runtime types
        if config.runtime_type == RuntimeType.CLUSTER and config.cluster_runtime:
            # Use model spec for ClusterServingRuntime
            model_spec = {
                "modelFormat": {
                    "name": config.cluster_runtime.model_format or config.framework.value
                },
                "runtime": config.cluster_runtime.runtime_name,
                "storageUri": config.storage_uri,
                "protocolVersion": config.protocol_version
            }
            if config.cluster_runtime.runtime_version:
                model_spec["modelFormat"]["version"] = config.cluster_runtime.runtime_version
            
            predictor_kwargs["model"] = model_spec
            
            # Add environment variables and resources to model spec
            if config.env_vars:
                env = [client.V1EnvVar(name=k, value=str(v)) for k, v in config.env_vars.items()]
                model_spec["env"] = env
            if resources:
                model_spec["resources"] = resources
                
        elif config.runtime_type == RuntimeType.BUILTIN:
            # Use framework-specific specs for built-in runtimes
            if config.framework == Framework.SKLEARN:
                framework_spec = V1beta1SKLearnSpec(
                    storage_uri=config.storage_uri,
                    protocol_version=config.protocol_version,
                    resources=resources
                )
                predictor_kwargs["sklearn"] = framework_spec
            elif config.framework == Framework.TENSORFLOW:
                framework_spec = V1beta1TensorflowSpec(
                    storage_uri=config.storage_uri,
                    protocol_version=config.protocol_version,
                    resources=resources
                )
                predictor_kwargs["tensorflow"] = framework_spec
            elif config.framework == Framework.PYTORCH:
                framework_spec = V1beta1PyTorchSpec(
                    storage_uri=config.storage_uri,
                    protocol_version=config.protocol_version,
                    resources=resources
                )
                predictor_kwargs["pytorch"] = framework_spec
            # Add other frameworks as needed
                
        elif config.runtime_type == RuntimeType.CUSTOM:
            if not config.custom_image:
                raise ValueError("custom_image is required for custom runtime type")
            
            env = []
            if config.env_vars:
                for key, value in config.env_vars.items():
                    env.append(client.V1EnvVar(name=key, value=str(value)))
            if config.storage_uri:
                env.append(client.V1EnvVar(name="STORAGE_URI", value=config.storage_uri))
            
            containers = [client.V1Container(
                name="kserve-container",
                image=config.custom_image,
                env=env if env else None,
                resources=resources
            )]
            predictor_kwargs["containers"] = containers
        
        predictor_spec = V1beta1PredictorSpec(**predictor_kwargs)
        
        # Create InferenceService spec
        isvc_spec = V1beta1InferenceServiceSpec(predictor=predictor_spec)
        
        # Create InferenceService
        isvc = V1beta1InferenceService(
            api_version="serving.kserve.io/v1beta1",
            kind="InferenceService",
            metadata=metadata,
            spec=isvc_spec
        )
        
        return isvc
    
    def template_batch(self, configs: List[InferenceServiceConfig], output_format: str = "yaml") -> List[str]:
        """
        Template multiple InferenceServices at once
        
        Args:
            configs: List of InferenceService configurations
            output_format: Output format ('yaml', 'dict', 'object')
            
        Returns:
            List of rendered templates
        """
        results = []
        for config in configs:
            if output_format == "yaml":
                result = self.render_inference_service_yaml(config)
            elif output_format == "dict":
                result = self.render_inference_service_dict(config)
            elif output_format == "object":
                result = self.create_inference_service_object(config)
            else:
                raise ValueError(f"Unsupported output format: {output_format}")
            results.append(result)
        return results
    
    def validate_template(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Validate the rendered template
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Validation result with status and any errors
        """
        try:
            # Try to render and parse
            yaml_str = self.render_inference_service_yaml(config)
            parsed = yaml.safe_load(yaml_str)
            
            # Basic validation
            errors = []
            if not parsed.get('metadata', {}).get('name'):
                errors.append("Missing metadata.name")
            if not parsed.get('spec', {}).get('predictor'):
                errors.append("Missing spec.predictor")
            if not config.storage_uri:
                errors.append("Missing storage_uri")
            
            return {
                "valid": len(errors) == 0,
                "errors": errors,
                "template": parsed
            }
        except Exception as e:
            return {
                "valid": False,
                "errors": [str(e)],
                "template": None
            }

# Helper functions
def create_cluster_runtime_config(name: str, storage_uri: str, runtime_name: str, 
                                  framework: Framework = Framework.SKLEARN, **kwargs) -> InferenceServiceConfig:
    """Helper to create ClusterServingRuntime configuration"""
    cluster_runtime = ClusterServingRuntimeConfig(
        runtime_name=runtime_name,
        **{k: v for k, v in kwargs.items() if k in ['runtime_version', 'protocol_version', 'model_format']}
    )
    
    return InferenceServiceConfig(
        name=name,
        storage_uri=storage_uri,
        framework=framework,
        runtime_type=RuntimeType.CLUSTER,
        cluster_runtime=cluster_runtime,
        **{k: v for k, v in kwargs.items() if k not in ['runtime_version', 'protocol_version', 'model_format']}
    )

def create_sklearn_config(name: str, storage_uri: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create sklearn configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.SKLEARN,
        storage_uri=storage_uri,
        **kwargs
    )

def create_custom_config(name: str, storage_uri: str, custom_image: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create custom configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.CUSTOM,
        runtime_type=RuntimeType.CUSTOM,
        storage_uri=storage_uri,
        custom_image=custom_image,
        **kwargs
    )

def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(description="KServe Template Generator")
    parser.add_argument("--name", required=True, help="InferenceService name")
    parser.add_argument("--storage-uri", required=True, help="Model storage URI")
    parser.add_argument("--framework", choices=[f.value for f in Framework], 
                       default="sklearn", help="ML framework")
    parser.add_argument("--runtime-type", choices=[r.value for r in RuntimeType],
                       default="cluster", help="Runtime type")
    parser.add_argument("--runtime-name", help="ClusterServingRuntime name")
    parser.add_argument("--namespace", default="default", help="Kubernetes namespace")
    parser.add_argument("--output-dir", help="Output directory for templates")
    parser.add_argument("--custom-image", help="Custom container image")
    
    args = parser.parse_args()
    
    # Initialize client
    client = KServeTemplatingClient(
        namespace=args.namespace,
        output_dir=args.output_dir
    )
    
    # Create configuration
    if args.runtime_type == "cluster":
        config = create_cluster_runtime_config(
            name=args.name,
            storage_uri=args.storage_uri,
            runtime_name=args.runtime_name or f"{args.framework}-server",
            framework=Framework(args.framework),
            namespace=args.namespace
        )
    elif args.runtime_type == "custom":
        if not args.custom_image:
            raise ValueError("--custom-image is required for custom runtime type")
        config = create_custom_config(
            name=args.name,
            storage_uri=args.storage_uri,
            custom_image=args.custom_image,
            namespace=args.namespace
        )
    else:  # builtin
        config = InferenceServiceConfig(
            name=args.name,
            storage_uri=args.storage_uri,
            framework=Framework(args.framework),
            runtime_type=RuntimeType.BUILTIN,
            namespace=args.namespace
        )
    
    # Generate and save template
    filepath = client.save_template_to_file(config)
    print(f"Template generated successfully: {filepath}")
    
    # Validate template
    validation = client.validate_template(config)
    if validation['valid']:
        print("✅ Template validation passed")
    else:
        print("❌ Template validation failed:")
        for error in validation['errors']:
            print(f"  - {error}")

if __name__ == "__main__":
    # Check if running as CLI
    import sys
    if len(sys.argv) > 1:
        main()
    else:
        # Example usage
        # Initialize the templating client
        client = KServeTemplatingClient(output_dir="./kserve-outputs")
        
        # Example 1: ClusterServingRuntime sklearn model
        sklearn_cluster_config = create_cluster_runtime_config(
            name="sklearn-iris-cluster",
            storage_uri="gs://kfserving-examples/models/sklearn/1.0/model",
            runtime_name="sklearn-server",
            framework=Framework.SKLEARN,
            namespace="default",
            deployment_mode=DeploymentMode.RAW_DEPLOYMENT,
            resources=ResourceConfig(
                cpu_request="200m",
                memory_request="1Gi",
                cpu_limit="1",
                memory_limit="2Gi"
            ),
            scaling=ScalingConfig(
                min_replicas=2,
                max_replicas=5
            ),
            annotations={
                "example.com/model-version": "1.0.0"
            }
        )
        
        # Render as YAML
        print("=== ClusterServingRuntime Sklearn Model YAML ===")
        print(client.render_inference_service_yaml(sklearn_cluster_config))
        print()
        
        # Save to file
        sklearn_path = client.save_template_to_file(sklearn_cluster_config)
        
        # Example 2: ClusterServingRuntime TensorFlow model with GPU
        tf_cluster_config = create_cluster_runtime_config(
            name="tensorflow-model-cluster",
            storage_uri="gs://kfserving-examples/models/tensorflow/flowers",
            runtime_name="tensorflow-serving",
            framework=Framework.TENSORFLOW,
            namespace="ml-models",
            deployment_mode=DeploymentMode.KNATIVE,
            env_vars={
                "TF_CPP_MIN_LOG_LEVEL": "2",
                "MODEL_NAME": "flowers"
            },
            resources=ResourceConfig(
                cpu_request="500m",
                memory_request="2Gi",
                cpu_limit="2",
                memory_limit="8Gi",
                gpu_limit="1"
            ),
            scaling=ScalingConfig(
                min_replicas=1,
                max_replicas=10,
                scale_target=10
            ),
            cluster_runtime=ClusterServingRuntimeConfig(
                runtime_name="tensorflow-serving",
                runtime_version="2.13.0",
                protocol_version="v1"
            )
        )
        
        print("=== ClusterServingRuntime TensorFlow Model YAML ===")
        print(client.render_inference_service_yaml(tf_cluster_config))
        print()
        
        # Save to file
        tf_path = client.save_template_to_file(tf_cluster_config)
        
        # Example 3: Custom model with storage URI
        custom_config = create_custom_config(
            name="custom-model-with-storage",
            storage_uri="s3://my-bucket/models/custom/",
            custom_image="myregistry/custom-model:v1.0",
            namespace="ml-models",
            deployment_mode=DeploymentMode.KNATIVE,
            env_vars={
                "MODEL_TYPE": "transformer",
                "BATCH_SIZE": "32",
                "LOG_LEVEL": "INFO"
            },
            resources=ResourceConfig(
                cpu_request="500m",
                memory_request="2Gi",
                cpu_limit="2",
                memory_limit="8Gi",
                gpu_limit="1"
            ),
            scaling=ScalingConfig(
                min_replicas=1,
                max_replicas=10,
                scale_target=10
            )
        )
        
        print("=== Custom Model with Storage URI YAML ===")
        print(client.render_inference_service_yaml(custom_config))
        print()
        
        # Save to file
        custom_path = client.save_template_to_file(custom_config)
        
        # Example 4: HuggingFace model with ClusterServingRuntime
        hf_config = create_cluster_runtime_config(
            name="huggingface-bert",
            storage_uri="hf://microsoft/DialoGPT-medium",
            runtime_name="huggingface-server",
            framework=Framework.HUGGINGFACE,
            namespace="default",
            env_vars={
                "HUGGINGFACE_HUB_CACHE": "/tmp/hf_cache",
                "TRANSFORMERS_CACHE": "/tmp/transformers_cache"
            },
            resources=ResourceConfig(
                cpu_request="1",
                memory_request="4Gi",
                cpu_limit="4",
                memory_limit="16Gi",
                gpu_limit="1"
            ),
            scaling=ScalingConfig(
                min_replicas=1,
                max_replicas=3
            )
        )
        
        print("=== HuggingFace Model with ClusterServingRuntime YAML ===")
        print(client.render_inference_service_yaml(hf_config))
        print()
        
        # Save to file
        hf_path = client.save_template_to_file(hf_config)
        
        # Example 5: Batch templating and saving
        all_configs = [sklearn_cluster_config, tf_cluster_config, custom_config, hf_config]
        
        print("=== Batch Processing ===")
        saved_files = client.save_batch_templates(all_configs)
        print(f"Saved {len(saved_files)} templates:")
        for file_path in saved_files:
            print(f"  - {file_path}")
        
        # Example 6: Validation
        print("\n=== Validation Results ===")
        for config in all_configs:
            validation_result = client.validate_template(config)
            status = "✅ VALID" if validation_result['valid'] else "❌ INVALID"
            print(f"{config.name}: {status}")
            if validation_result['errors']:
                for error in validation_result['errors']:
                    print(f"  - {error}")
        
        # Example 7: PyTorch model with advanced configuration
        pytorch_config = create_cluster_runtime_config(
            name="pytorch-resnet",
            storage_uri="gs://my-bucket/pytorch-models/resnet50/",
            runtime_name="pytorch-server",
            framework=Framework.PYTORCH,
            namespace="production",
            deployment_mode=DeploymentMode.RAW_DEPLOYMENT,
            env_vars={
                "PYTORCH_JIT": "1",
                "OMP_NUM_THREADS": "4"
            },
            resources=ResourceConfig(
                cpu_request="1",
                memory_request="4Gi",
                cpu_limit="4",
                memory_limit="16Gi",
                gpu_limit="1"
            ),
            scaling=ScalingConfig(
                min_replicas=2,
                max_replicas=8,
                scale_target=50,
                scale_metric="concurrency"
            ),
            annotations={
                "serving.kserve.io/autoscalerClass": "hpa.autoscaling.knative.dev",
                "serving.kserve.io/metric": "concurrency",
                "serving.kserve.io/targetUtilizationPercentage": "75"
            },
            labels={
                "app": "pytorch-inference",
                "version": "v1.0",
                "environment": "production"
            },
            service_account="pytorch-inference-sa",
            node_selector={
                "node-type": "gpu-node",
                "zone": "us-west1-a"
            },
            tolerations=[
                {
                    "key": "nvidia.com/gpu",
                    "operator": "Equal",
                    "value": "present",
                    "effect": "NoSchedule"
                }
            ]
        )
        
        print("\n=== Advanced PyTorch Configuration YAML ===")
        print(client.render_inference_service_yaml(pytorch_config))
        
        # Save advanced config
        pytorch_path = client.save_template_to_file(pytorch_config, "pytorch_advanced_config.yaml")
        print(f"\nAdvanced PyTorch config saved to: {pytorch_path}")
        
        print(f"\n🎉 All examples completed! Check the '{client.output_dir}' directory for generated templates.")
