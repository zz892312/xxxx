# Set your project first
runai project set <your-project-name>

# Submit an inference workload with ONNX Runtime
runai submit onnx-inference \
  --image mcr.microsoft.com/onnxruntime/server:latest \
  --gpu 1 \
  --service-type nodeport \
  --port 8001 \
  --volume /path/to/your/model:/models \
  --env MODEL_PATH=/models/your-model.onnx \
  --command -- python -c "
import onnxruntime as ort
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import numpy as np

class ONNXHandler(BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.session = ort.InferenceSession('/models/your-model.onnx')
        super().__init__(*args, **kwargs)
    
    def do_POST(self):
        # Handle inference requests
        pass

httpd = HTTPServer(('0.0.0.0', 8001), ONNXHandler)
httpd.serve_forever()
"
# Create a YAML file for your ONNX inference workload
cat << EOF > onnx-inference.yaml
apiVersion: run.ai/v2alpha1
kind: Workload
metadata:
  name: onnx-inference
spec:
  workloadType: inference
  image: mcr.microsoft.com/onnxruntime/server:latest
  gpu:
    value: 1
  ports:
    - containerPort: 8001
      serviceType: nodePort
  volumes:
    - name: model-volume
      hostPath: /path/to/your/model
      containerPath: /app/model
  command: ["python", "-m", "onnxruntime.tools.server", "--model_path", "/app/model/your-model.onnx", "--host", "0.0.0.0", "--port", "8001"]
EOF

# Submit the workload
kubectl apply -f onnx-inference.yaml
