import yaml
import json
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, asdict
from jinja2 import Template, Environment, BaseLoader
from kubernetes import client
from kserve import KServeClient
from kserve import V1beta1InferenceService
from kserve import V1beta1InferenceServiceSpec
from kserve import V1beta1PredictorSpec
from kserve import V1beta1SKLearnSpec
from kserve import V1beta1TensorflowSpec
from kserve import V1beta1PyTorchSpec
from kserve import V1beta1XGBoostSpec
from kserve import V1beta1LightGBMSpec
from enum import Enum

class DeploymentMode(Enum):
    KNATIVE = "Knative"
    RAW_DEPLOYMENT = "RawDeployment"

class Framework(Enum):
    SKLEARN = "sklearn"
    TENSORFLOW = "tensorflow"
    PYTORCH = "pytorch"
    XGBOOST = "xgboost"
    LIGHTGBM = "lightgbm"
    CUSTOM = "custom"
    HUGGINGFACE = "huggingface"

@dataclass
class ResourceConfig:
    cpu_request: str = "100m"
    memory_request: str = "512Mi"
    cpu_limit: str = "1"
    memory_limit: str = "2Gi"
    gpu_limit: Optional[str] = None

@dataclass
class ScalingConfig:
    min_replicas: int = 1
    max_replicas: int = 3
    scale_target: Optional[int] = None
    scale_metric: str = "concurrency"

@dataclass
class InferenceServiceConfig:
    name: str
    namespace: str = "default"
    framework: Framework = Framework.SKLEARN
    storage_uri: str = ""
    deployment_mode: DeploymentMode = DeploymentMode.KNATIVE
    protocol_version: str = "v1"
    custom_image: Optional[str] = None
    env_vars: Optional[Dict[str, str]] = None
    resources: ResourceConfig = None
    scaling: ScalingConfig = None
    annotations: Optional[Dict[str, str]] = None
    labels: Optional[Dict[str, str]] = None
    service_account: Optional[str] = None
    node_selector: Optional[Dict[str, str]] = None
    tolerations: Optional[List[Dict[str, Any]]] = None
    canary_traffic_percent: Optional[int] = None
    transformer_config: Optional[Dict[str, Any]] = None
    explainer_config: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.resources is None:
            self.resources = ResourceConfig()
        if self.scaling is None:
            self.scaling = ScalingConfig()
        if isinstance(self.framework, str):
            self.framework = Framework(self.framework)
        if isinstance(self.deployment_mode, str):
            self.deployment_mode = DeploymentMode(self.deployment_mode)

class KServeTemplatingClient:
    def __init__(self, namespace: str = "default"):
        """
        Initialize KServe Templating Client
        
        Args:
            namespace: Default Kubernetes namespace
        """
        self.namespace = namespace
        self.kserve_client = None
        self._init_templates()
    
    def _init_templates(self):
        """Initialize Jinja2 templates for different components"""
        self.templates = {
            'inference_service': """
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ config.name }}
  namespace: {{ config.namespace }}
  {% if annotations %}
  annotations:
    {% for key, value in annotations.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if labels %}
  labels:
    {% for key, value in labels.items() %}
    {{ key }}: "{{ value }}"
    {% endfor %}
  {% endif %}
spec:
  {% if config.service_account %}
  serviceAccountName: {{ config.service_account }}
  {% endif %}
  predictor:
    {% if config.scaling.min_replicas %}
    minReplicas: {{ config.scaling.min_replicas }}
    {% endif %}
    {% if config.scaling.max_replicas %}
    maxReplicas: {{ config.scaling.max_replicas }}
    {% endif %}
    {% if config.scaling.scale_target %}
    scaleTarget: {{ config.scaling.scale_target }}
    {% endif %}
    {% if config.scaling.scale_metric %}
    scaleMetric: {{ config.scaling.scale_metric }}
    {% endif %}
    {% if config.node_selector %}
    nodeSelector:
      {% for key, value in config.node_selector.items() %}
      {{ key }}: "{{ value }}"
      {% endfor %}
    {% endif %}
    {% if config.tolerations %}
    tolerations:
      {% for toleration in config.tolerations %}
      - {{ toleration | tojson }}
      {% endfor %}
    {% endif %}
    {{ framework_spec | indent(4, true) }}
  {% if transformer_spec %}
  transformer:
    {{ transformer_spec | indent(4, true) }}
  {% endif %}
  {% if explainer_spec %}
  explainer:
    {{ explainer_spec | indent(4, true) }}
  {% endif %}
""",
            'sklearn_spec': """
sklearn:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'tensorflow_spec': """
tensorflow:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'pytorch_spec': """
pytorch:
  storageUri: {{ config.storage_uri }}
  protocolVersion: {{ config.protocol_version }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
""",
            'custom_spec': """
containers:
- name: kserve-container
  image: {{ config.custom_image }}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
    {% if config.storage_uri %}
    - name: STORAGE_URI
      value: {{ config.storage_uri }}
    {% endif %}
  {% endif %}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
""",
            'huggingface_spec': """
huggingface:
  storageUri: {{ config.storage_uri }}
  {% if resources %}
  resources:
    requests:
      cpu: {{ config.resources.cpu_request }}
      memory: {{ config.resources.memory_request }}
    limits:
      cpu: {{ config.resources.cpu_limit }}
      memory: {{ config.resources.memory_limit }}
      {% if config.resources.gpu_limit %}
      nvidia.com/gpu: {{ config.resources.gpu_limit }}
      {% endif %}
  {% endif %}
  {% if env_vars %}
  env:
    {% for key, value in env_vars.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
"""
        }
        
        # Initialize Jinja2 environment
        self.jinja_env = Environment(loader=BaseLoader())
        self.jinja_env.filters['tojson'] = json.dumps
    
    def _render_framework_spec(self, config: InferenceServiceConfig) -> str:
        """Render framework-specific specification"""
        template_key = f"{config.framework.value}_spec"
        
        if template_key not in self.templates:
            raise ValueError(f"Unsupported framework: {config.framework.value}")
        
        template = self.jinja_env.from_string(self.templates[template_key])
        return template.render(
            config=config,
            env_vars=config.env_vars,
            resources=config.resources is not None
        )
    
    def _render_transformer_spec(self, transformer_config: Dict[str, Any]) -> str:
        """Render transformer specification"""
        if not transformer_config:
            return ""
        
        # Simple transformer template
        template_str = """
containers:
- name: transformer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**transformer_config)
    
    def _render_explainer_spec(self, explainer_config: Dict[str, Any]) -> str:
        """Render explainer specification"""
        if not explainer_config:
            return ""
        
        # Simple explainer template
        template_str = """
containers:
- name: explainer
  image: {{ image }}
  {% if env %}
  env:
    {% for key, value in env.items() %}
    - name: {{ key }}
      value: "{{ value }}"
    {% endfor %}
  {% endif %}
  {% if resources %}
  resources:
    {{ resources | tojson | indent(4) }}
  {% endif %}
"""
        template = self.jinja_env.from_string(template_str)
        return template.render(**explainer_config)
    
    def render_inference_service_yaml(self, config: InferenceServiceConfig) -> str:
        """
        Render InferenceService as YAML string
        
        Args:
            config: InferenceService configuration
            
        Returns:
            str: Rendered YAML string
        """
        # Prepare annotations
        annotations = config.annotations or {}
        if config.deployment_mode == DeploymentMode.RAW_DEPLOYMENT:
            annotations["serving.kserve.io/deploymentMode"] = "RawDeployment"
        
        # Render framework specification
        framework_spec = self._render_framework_spec(config)
        
        # Render transformer and explainer specs if provided
        transformer_spec = self._render_transformer_spec(config.transformer_config) if config.transformer_config else ""
        explainer_spec = self._render_explainer_spec(config.explainer_config) if config.explainer_config else ""
        
        # Render main template
        template = self.jinja_env.from_string(self.templates['inference_service'])
        rendered = template.render(
            config=config,
            annotations=annotations if annotations else None,
            labels=config.labels,
            framework_spec=framework_spec,
            transformer_spec=transformer_spec if transformer_spec else None,
            explainer_spec=explainer_spec if explainer_spec else None
        )
        
        return rendered.strip()
    
    def render_inference_service_dict(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Render InferenceService as Python dictionary
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Rendered InferenceService as dictionary
        """
        yaml_str = self.render_inference_service_yaml(config)
        return yaml.safe_load(yaml_str)
    
    def create_inference_service_object(self, config: InferenceServiceConfig) -> V1beta1InferenceService:
        """
        Create KServe InferenceService object from configuration
        
        Args:
            config: InferenceService configuration
            
        Returns:
            V1beta1InferenceService: KServe InferenceService object
        """
        isvc_dict = self.render_inference_service_dict(config)
        
        # Convert dictionary to KServe object
        # This is a simplified conversion - you might need more complex logic for all cases
        metadata = client.V1ObjectMeta(
            name=config.name,
            namespace=config.namespace,
        )
        
        # Add annotations if any
        annotations = config.annotations or {}
        if config.deployment_mode == DeploymentMode.RAW_DEPLOYMENT:
            annotations["serving.kserve.io/deploymentMode"] = "RawDeployment"
        if annotations:
            metadata.annotations = annotations
        
        # Add labels if any
        if config.labels:
            metadata.labels = config.labels
        
        # Create predictor spec based on framework
        predictor_kwargs = {
            "min_replicas": config.scaling.min_replicas,
            "max_replicas": config.scaling.max_replicas,
        }
        
        # Add resources if specified
        resources = None
        if config.resources:
            resources = client.V1ResourceRequirements(
                requests={
                    "cpu": config.resources.cpu_request,
                    "memory": config.resources.memory_request
                },
                limits={
                    "cpu": config.resources.cpu_limit,
                    "memory": config.resources.memory_limit
                }
            )
            if config.resources.gpu_limit:
                resources.limits["nvidia.com/gpu"] = config.resources.gpu_limit
        
        # Create framework-specific spec
        if config.framework == Framework.SKLEARN:
            framework_spec = V1beta1SKLearnSpec(
                storage_uri=config.storage_uri,
                protocol_version=config.protocol_version,
                resources=resources
            )
            predictor_kwargs["sklearn"] = framework_spec
        elif config.framework == Framework.TENSORFLOW:
            framework_spec = V1beta1TensorflowSpec(
                storage_uri=config.storage_uri,
                protocol_version=config.protocol_version,
                resources=resources
            )
            predictor_kwargs["tensorflow"] = framework_spec
        elif config.framework == Framework.PYTORCH:
            framework_spec = V1beta1PyTorchSpec(
                storage_uri=config.storage_uri,
                protocol_version=config.protocol_version,
                resources=resources
            )
            predictor_kwargs["pytorch"] = framework_spec
        elif config.framework == Framework.CUSTOM:
            if not config.custom_image:
                raise ValueError("custom_image is required for custom framework")
            
            env = []
            if config.env_vars:
                for key, value in config.env_vars.items():
                    env.append(client.V1EnvVar(name=key, value=str(value)))
            if config.storage_uri:
                env.append(client.V1EnvVar(name="STORAGE_URI", value=config.storage_uri))
            
            containers = [client.V1Container(
                name="kserve-container",
                image=config.custom_image,
                env=env if env else None,
                resources=resources
            )]
            predictor_kwargs["containers"] = containers
        
        predictor_spec = V1beta1PredictorSpec(**predictor_kwargs)
        
        # Create InferenceService spec
        isvc_spec = V1beta1InferenceServiceSpec(predictor=predictor_spec)
        
        # Create InferenceService
        isvc = V1beta1InferenceService(
            api_version="serving.kserve.io/v1beta1",
            kind="InferenceService",
            metadata=metadata,
            spec=isvc_spec
        )
        
        return isvc
    
    def template_batch(self, configs: List[InferenceServiceConfig], output_format: str = "yaml") -> List[str]:
        """
        Template multiple InferenceServices at once
        
        Args:
            configs: List of InferenceService configurations
            output_format: Output format ('yaml', 'dict', 'object')
            
        Returns:
            List of rendered templates
        """
        results = []
        for config in configs:
            if output_format == "yaml":
                result = self.render_inference_service_yaml(config)
            elif output_format == "dict":
                result = self.render_inference_service_dict(config)
            elif output_format == "object":
                result = self.create_inference_service_object(config)
            else:
                raise ValueError(f"Unsupported output format: {output_format}")
            results.append(result)
        return results
    
    def save_template_to_file(self, config: InferenceServiceConfig, filepath: str):
        """
        Save rendered template to file
        
        Args:
            config: InferenceService configuration
            filepath: Path to save the file
        """
        yaml_content = self.render_inference_service_yaml(config)
        with open(filepath, 'w') as f:
            f.write(yaml_content)
    
    def validate_template(self, config: InferenceServiceConfig) -> Dict[str, Any]:
        """
        Validate the rendered template
        
        Args:
            config: InferenceService configuration
            
        Returns:
            dict: Validation result with status and any errors
        """
        try:
            # Try to render and parse
            yaml_str = self.render_inference_service_yaml(config)
            parsed = yaml.safe_load(yaml_str)
            
            # Basic validation
            errors = []
            if not parsed.get('metadata', {}).get('name'):
                errors.append("Missing metadata.name")
            if not parsed.get('spec', {}).get('predictor'):
                errors.append("Missing spec.predictor")
            
            return {
                "valid": len(errors) == 0,
                "errors": errors,
                "template": parsed
            }
        except Exception as e:
            return {
                "valid": False,
                "errors": [str(e)],
                "template": None
            }

# Example usage and helper functions
def create_sklearn_config(name: str, storage_uri: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create sklearn configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.SKLEARN,
        storage_uri=storage_uri,
        **kwargs
    )

def create_custom_config(name: str, custom_image: str, **kwargs) -> InferenceServiceConfig:
    """Helper to create custom configuration"""
    return InferenceServiceConfig(
        name=name,
        framework=Framework.CUSTOM,
        custom_image=custom_image,
        **kwargs
    )

if __name__ == "__main__":
    # Initialize the templating client
    client = KServeTemplatingClient()
    
    # Example 1: Simple sklearn model
    sklearn_config = InferenceServiceConfig(
        name="sklearn-iris",
        namespace="default",
        framework=Framework.SKLEARN,
        storage_uri="gs://kfserving-examples/models/sklearn/1.0/model",
        deployment_mode=DeploymentMode.RAW_DEPLOYMENT,
        resources=ResourceConfig(
            cpu_request="200m",
            memory_request="1Gi",
            cpu_limit="1",
            memory_limit="2Gi"
        ),
        scaling=ScalingConfig(
            min_replicas=2,
            max_replicas=5
        ),
        annotations={
            "example.com/model-version": "1.0.0"
        }
    )
    
    # Render as YAML
    print("=== Sklearn Model YAML ===")
    print(client.render_inference_service_yaml(sklearn_config))
    print()
    
    # Example 2: Custom model with GPU
    custom_config = InferenceServiceConfig(
        name="custom-model",
        namespace="ml-models",
        framework=Framework.CUSTOM,
        custom_image="myregistry/custom-model:v1.0",
        storage_uri="s3://my-bucket/models/custom/",
        deployment_mode=DeploymentMode.KNATIVE,
        env_vars={
            "MODEL_TYPE": "transformer",
            "BATCH_SIZE": "32"
        },
        resources=ResourceConfig(
            cpu_request="500m",
            memory_request="2Gi",
            cpu_limit="2",
            memory_limit="8Gi",
            gpu_limit="1"
        ),
        scaling=ScalingConfig(
            min_replicas=1,
            max_replicas=10,
            scale_target=10
        )
    )
    
    print("=== Custom Model YAML ===")
    print(client.render_inference_service_yaml(custom_config))
    print()
    
    # Example 3: Batch templating
    configs = [sklearn_config, custom_config]
    templates = client.template_batch(configs, output_format="dict")
    
    print("=== Batch Templates (Count) ===")
    print(f"Generated {len(templates)} templates")
    
    # Example 4: Validation
    validation_result = client.validate_template(sklearn_config)
    print("=== Validation Result ===")
    print(f"Valid: {validation_result['valid']}")
    if validation_result['errors']:
        print(f"Errors: {validation_result['errors']}")


can you edit above I want to leverage the most of the clusterservingruntime 
storage uri will be input 
add logic to save template to output dir
